{
  "hash": "8642bb72c533dcf0a745157599ad59eb",
  "result": {
    "engine": "knitr",
    "markdown": "# Models {#sec-model}\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nBy the end of this chapter you should gain the following knowledge and practical skills.\n\n::: {.callout-note}\n\n## Knowledge\n\n- [ ] Be reminded of the basics of linear regression modelling.\n- [ ] Appreciate how data graphics can inform the process of building and evaluating models. \n- [ ] Understand two categories of geographic effect in regression modelling: spatial *dependence* in values and spatial  *non-stationarity* in processes.\n- [ ] Learn how graphics can be used to test for these effects and how linear regression models can be updated to account for and further explore them.\n\n:::\n\n\n::: {.callout-note}\n\n## Practical skills\n\n- [ ] Write ggplot2 code to generate graphics for exploring multivariate association and presenting regression outputs (faceted scatterplots, parallel coordinate plots, dot plots with error bars).\n- [ ] Write code to generate linear regression models in R.\n- [ ] Extract model outputs and diagnostics in a `tidy` manner.\n- [ ] Apply functional-style programming for working over multiple model outputs.\n- [ ] Generate graphical line-up plots in ggplot2 to test regression assumptions.\n:::\n\n## Introduction\n\n\\index{elections}\nSo far the analysis presented in this book has been data-driven. Having described data in a consistent way ([@sec-data]), visual analysis approaches have been applied, informed by established visualization guidelines. Chapters [-@sec-explore] and [-@sec-network] involved model building, but these were largely value-free models based on limited prior theory. This chapter is presented as a worked data analysis. We look at a well-known dataset with a more explicit and theoretically-informed motivation.\n\nThe chapter explores variation in voting behaviour in the \\index{Brexit|(} UK's 2016 referendum on leaving the EU. You might remember that while there was a slight majority for Leave (c. 52%), the vote varied between different parts of the country. There were many explanations offered for why particular places voted the way they did, often related to the demographic composition of those areas. We will explore whether the discussed compositional demographic factors vary systematically with area-level Leave voting. Using regression frameworks, we will model the relative effect of each of these compositional factors in structuring variation in the vote and construct data graphics that allow these models and parameters to be evaluated in detail. \\index{regression modelling|(}\n\n\n\n::: {.callout-note}\n\n## Regression primer\n This chapter assumes some basic familiarity with linear regression modelling. For a fuller overview, with excellent and real-world social science examples, you may wish to consult *Regression and Other Stories* [@gelman_regression_2020].\n:::\n\n## Concepts\n\n### Quantifying and exploring variation\n\n<!-- Variation is central to most data analysis, and certainly regression modelling: quantifying variation, exploring how it is structured and accounting for (or explaining) it using a combination of empirical data, prior theory and knowledge. -->\n\n\\index{datasets!EU referendum|(}\n\n\nIn @fig-map-uniform is a map and bar chart of voting in the 2016 EU referendum, estimated at Parliamentary Constituency level \\index{parliamentary constituency} [see @hanretty_areal_2017]. The values themselves are the difference in estimated vote shares from an expectation that the Leave vote by constituency, $y_{i}$ our *outcome* \\index{outcome variable} of interest, is uniformly distributed across the country and so equivalent to the overall Great Britain (GB) vote share for Leave of c. 52%. Although a slightly contrived formulation, we could express this as an intercept-only \\index{regression modelling!intercept-only model|(} linear regression model, where the estimated *slope* ($\\beta_{1}$) is 'turned off' (takes the value $0$) and the *intercept* ($\\beta_{0}$) is the GB average vote share for Leave ($\\bar{y}$):\n\n\\begin{align*}\n       y_{i}= \\beta_{0} + \\beta_{1} + \\varepsilon_{i}\n\\end{align*}\n\nSo we estimate the Leave vote in each constituency ($y_{i}$) as a function of:\n\n* $\\beta_{0}$, the intercept, the GB average vote share ($\\bar{y}$) $+$\n* $\\beta_{1}=0$, a negated slope, $+$\n* $\\varepsilon_{i}$, a statistical error term capturing the difference between $y_{i}$, the observed Leave vote in a constituency, and the unobservable 'true population' value of the Leave vote in each constituency\n\nHow does this relate to the idea of characterising variation? The length and colour of each bar in @fig-map-uniform is scaled according to model \\index{residuals} *residuals*: the difference between $y_{i}$, the observed value, and the expected value of the Leave vote  under the uniform model. The sum of these bar lengths is therefore the total variance that we later try to account for by updating our regression model to generate new expected values using information on the demographic composition of constituencies.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Residuals from uniform model comparing constituency Leave vote to GB average.](figs/06/map_uniform.png){#fig-map-uniform width=100%}\n:::\n:::\n\n\n\n\n\n@fig-map-uniform is similar to the maps that were published widely in press reports in the aftermath of the vote, and demonstrates that there is indeed substantial variation in Leave voting between different parts of the country. The intercept-only model  consistently underestimates the vote in Scotland and most of London. Outside of this, constituencies voting in smaller proportions than would be expected for Leave are distributed more sparsely in the country: the dark red dot with surrounding red area in the east of England is Cambridge and Cambridgeshire, constituencies in Bristol (south west), Manchester and Liverpool (north west) and Brighton (south) are also reasonably strong red.\n\n\\index{regression modelling!assumptions}\nWhen evaluating the effectiveness of modelled values, there are various checks that can be performed. A relevant check here is whether there is bias in the residuals -- whether residuals have structure that suggests they are grouped in a way not captured by the model. Given the motivation behind our analysis, it is no surprise that there is a geographic pattern to the residuals in @fig-map-uniform, but also the non-symmetrical shape of the 'signed' bars in the left of the graphic. There are more constituencies with positive values than negative; the Leave vote is underestimated by the uniform model for 57% of constituencies, and some constituencies have quite large negative values. The strongest vote for Leave was Boston and Skegness with 76% for Leave, but the strongest for Remain was Hackney North and Stoke Newington with 80% for Remain.\n\n\\index{regression modelling!intercept-only model|)}\n\n### Quantifying and exploring co-variation\n\nMore interesting still is whether the pattern of variation in @fig-map-uniform is correlated with compositional factors that we think explain this variation; and whether bias or structure in residuals exists even after accounting for these compositional factors.\n\n\\index{datasets!Census}\nIn [@tbl-variables]{.content-visible when-format=\"html\"} [@tbl-variables-tex]{.content-visible when-format=\"pdf\"} is a list of possible explanatory variables describing the demographic composition of constituencies.  Each variable is expressed as a proportion of the constituency's population. So the *degree-educated* variable describes the proportion of residents in the constituency educated at least to degree-level. Comparison across these variables is challenging due to the fact that their ranges differ: the *EU-born* variable ranges from 0.6% to 17%; the *white* variable from 14% to 98%. Common practice for addressing these sorts of range problem is to z-score transform the variables so that each is expressed in standard deviation units from its mean. \\index{z-score transformation}\n\n<!-- ```{r}\n#| label: tbl-variables\n#| tbl-cap: \"Breakdown of variable types.\"\n#| echo: false\n#| eval: !expr knitr::is_html_output()\n\nvars <- tibble::tibble(\n  `Census variable` = c(\"degree-educated\", \"professional occupations\", \"younger adults\",\n  \"heavy industry\", \"not good health\", \"white\", \"Christian\", \"EU-born\",\n  \"own home\", \"no car\"),\n  `Constituency %` = c(\"with degrees +\", \"ns-sec manager/professional\", \"adults aged <44\",\n  \"manufacturing and transport\", \"reported fair, bad, very bad\", \"ethnicity white British/Irish\", \"Christian\", \"EU-born (not UK)\",\n  \"own home\", \"don't own a car\")\n    )\n  kbl(vars) |>\n      pack_rows(\"post-industrial / knowledge economy\", 1, 4, bold=FALSE, label_row_css = \"border-bottom: 0px solid;\") |>\n      pack_rows(\"diversity/values/outcomes\", 5, 8, bold=FALSE, label_row_css = \"border-bottom: 0px solid;\") |>\n      pack_rows(\"metropolitan / 'big city'\", 9, 10, bold=FALSE, label_row_css = \"border-bottom: 0px solid;\") |>\n      column_spec(1, width = \"60%\") |>\n      row_spec(0, extra_css = \"border-bottom: 1px solid;\")\n``` -->\n\n\n::: {#tbl-variables-tex .cell tbl-cap='Breakdown of variable types.'}\n::: {.cell-output-display}\n\\begin{table}\n\\centering\\begingroup\\fontsize{9}{11}\\selectfont\n\n\\begin{tabular}[t]{l|l}\n\\hline\nCensus variable & Constituency \\%\\\\\n\\hline\n\\multicolumn{2}{l}{post-industrial / knowledge economy}\\\\\n\\hline\n\\hspace{1em}degree-educated & with degrees +\\\\\n\\hline\n\\hspace{1em}professional occupations & ns-sec manager/professional\\\\\n\\hline\n\\hspace{1em}younger adults & adults aged <44\\\\\n\\hline\n\\hspace{1em}heavy industry & manufacturing and transport\\\\\n\\hline\n\\multicolumn{2}{l}{diversity/values/outcomes}\\\\\n\\hline\n\\hspace{1em}not good health & reported fair, bad, very bad\\\\\n\\hline\n\\hspace{1em}white & ethnicity white British/Irish\\\\\n\\hline\n\\hspace{1em}Christian & Christian\\\\\n\\hline\n\\hspace{1em}EU-born & EU-born (not UK)\\\\\n\\hline\n\\multicolumn{2}{l}{metropolitan / 'big city'}\\\\\n\\hline\n\\hspace{1em}own home & own home\\\\\n\\hline\n\\hspace{1em}no car & don't own a car\\\\\n\\hline\n\\end{tabular}\n\\endgroup{}\n\\end{table}\n\n\n:::\n:::\n\n\n\n\\index{multivariate plots!scatterplots|(}\n@fig-scatters presents scatterplots from which the extent of linear association between these demographics and Leave voting in each constituency can be inferred. Each dot is a constituency, arranged on the x-axis according to the value of the explanatory variable and the y-axis according to the share of Leave vote. The scatterplots are faceted by explanatory variable and ordered left-to-right and top-to-bottom according to correlation coefficient. The variable most heavily correlated with Leave voting is *degree-education*: as the share of a constituency's population educated at least to *degree-level* increases, the share of Leave vote in that constituency decreases. An association in the same direction, but to a lesser extent, is observed for variables representing similar concepts:  *professional occupations*, *younger adults*, *EU-born*, *no-car* and the reverse for *Christian*, *not-good health* and *heavy industry*.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatterplots of constituency Leave vote against selected explanatory variables.](figs/06/scatters.png){#fig-scatters width=90%}\n:::\n:::\n\n\n\n\n\\index{multivariate plots!scatterplots|)}\n\n\\index{multivariate plots!parallel coordinate plots|(}\nIt is of course likely that many of the compositional characteristics of constituencies vary with Leave voting in consistent ways. Parallel coordinate plots may help visually explore this multivariate space. Whereas in scatterplots observations are represented as points located in x- and y- axes that are orthogonal, in a parallel coordinate plot observations are laid out across many parallel axes and the values of each observation  encoded via a line connecting the multiple parallel axes. In @fig-pcps each of the thin lines is a constituency coloured according to the recorded voting outcome -- either majority Remain (red) or Leave (blue). The first variable encoded is the size of the Leave vote, and variables are then ordered on their linear association with Leave. Note that we have reversed the polarity of variables such as *degree-educated* and *professional* so that we expect more Leave (blue lines) towards the right locations of the parallel axes. That the blue and red lines are reasonably separated suggests that there is a consistent pattern of association across many of the demographics characteristics in constituencies voting differently on Leave and Remain.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Parallel coordinate plot of constituency Leave vote and selected explanatory variables.](figs/06/pcps.png){#fig-pcps width=80%}\n:::\n:::\n\n\n\n\n\n::: {.callout-note}\n\n## On parallel coordinate plots\n\\index{data visualization!trade-offs}\nAlthough parallel coordinate plots enable some aspects of association between multiple variables to be inferred, they have several deficiencies. Association can only be directly inferred by comparing variables that are immediately adjacent. The order of parallel variables can greatly affect their visual appearance. And the corollary is that visual patterns of the plot that are salient may be incidental to the statistical features being inferred.\n:::\n\n<!--\n::: {.callout-note}\n\n You will remember from introductory stats courses that that the correlation coefficient can be used to summarise the strength of linear association between two variables. It is a quantity that ranges from perfect negative correlation, -1 -- as one value increases another decreases in the same proportion -- to perfect positive correlation, +1 -- as one value increases another increases in the same proportion. A value of 0 indicates no association -- the values increase and decrease independently of each other.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatterplots of synthetic bivariate data with extent of correlation coefficient systematically varied](figs/06/cors.png){#fig-cors width=100%}\n:::\n:::\n\n\n\n\n::: -->\n\n\\index{multivariate plots!parallel coordinate plots|)}\n\n### Modelling for co-variation\n\n\\index{regression modelling!multivariate regression|(}\n\nLinear regression provides a framework for systematically describing the associations implied by the scatterplots and parallel coordinate plot, and with respect to the constituency-level variation identified in @fig-map-uniform. Having seen these data, the demographic variables in @fig-scatters, we can derive new expected values of constituency-level Leave voting.\n\nTo express this in equation form, we update the uniform model such that Leave vote is a *function* of the selected explanatory variables. For single-variable linear regression,  we might select the proportion of residents educated at least to *degree-level* (${d_{i1}}$):\n\n\\begin{align*}\n       y_{i}&= \\beta_{0} + \\beta_{1}{d_{i1}} + \\varepsilon_{i}  \\\\\n\\end{align*}\n\nSo we now estimate the Leave vote in each constituency ($y_{i}$) as a function of:\n\n* $\\beta_{0}$, the intercept, the GB average vote share ($\\bar{y}$) $+$\n* $\\beta_{1}=\\beta_{1}{d_{i1}}$, the slope, indicating in which direction and to what extent *degree-educated* is associated with Leave,  $+$\n* $\\varepsilon_{i}$, the difference between $y_{i}$ (the observed value) and the unobservable 'true population' value of the Leave vote in that constituency (statistical error)\n\n<!-- There are different algorithms that can be used to estimate these parameters. Most obvious is ordinary least squares (OLS), which aims to minimise the sum of the (squared) residuals between the observed Leave vote in a constituency, $y_{i}$, and that expected given the association with the *degree-educated* explanatory variable $d_{i1}$.  -->\n\n<!-- Eyeballing the scatterplots in @fig-scatters we can already infer many of the parameters estimated via OLS regression. *Degree-educated* is most consistently associated with Leave voting and, according to our linear regression model, explains 60% of the total variation (the sum of bars in @fig-map-uniform) in constituency-level Leave voting. Also from the scatterplots, especially those associating Leave with *degree-educated* and *professionals*, there is a grouping of constituencies where the Leave vote is lower than we might expect given that constituency's population characteristics -- and this is reflected when inspecting the 1D distribution of residuals in histograms (not included here). -->\n\nIt is of course likely that some demographic variables account for different elements of variation in the Leave vote than others. You will be aware that the linear regression model can be extended to include many explanatory variables:\n\n\\begin{align*}\n      y_{i}&= \\beta_{0} +\\beta_{1}x_{i1} + ... + \\beta_{k}x_{ik} + \\varepsilon_{i}  \\\\\n\\end{align*}\n\nSo this results in *separate* $\\beta_{k}$ coefficients for separate explanatory variables. These coefficients can be interpreted as the degree of association between the explanatory variable $k$ and the outcome variable, keeping all the other explanatory variables constant -- or the distinct correlation between an explanatory variable $k$ and the outcome variable, net of the other variables included in the model.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Outputs from multiple regression model of Leave vote by demographic composition of constituency.](figs/06/outputs.png){#fig-outputs width=85%}\n:::\n:::\n\n\n\n\nIn @fig-outputs are regression coefficients ($\\beta_{k}$) from a multiple regression model with *degree-educated*, *no car*, *white*, *heavy industry*, *EU-born* and *not good health* selected as explanatory variables. Coefficients are reported as dots with estimates of uncertainty represented as lines encoding 95% \\index{confidence intervals} confidence intervals.\n<!-- the range of values the true (*unobservable*) coefficient is likely to take --> Most variables' coefficients are in the direction that would be expected given the associations in @fig-scatters. Net of variation in the other compositional factors, increased levels of *degree-education* in a constituency has the effect of reducing the Leave vote. The two exceptions are *EU-born*  and *white*: after controlling for variation in the other demographic variables, increased proportions of *white* residents reduces the Leave vote, and increased proportions of residents that are *EU-born* increases the Leave vote. Since the \\index{confidence intervals} confidence interval for *white* crosses zero, this coefficient is subject to much uncertainty. Further exploration may allow us to identify whether these counter-intuitive effects are genuine or the result of a poorly-specified model.\n\n\\index{regression modelling!multivariate regression|)}\n\n\n\n### Evaluating model bias\n\n\\index{regression modelling!assumptions|(} \\index{model bias}\n\nOur analysis becomes more interesting when we start to explore and characterise model *bias*: any underlying structure to the observations that is less well accounted for by the model.\n\nFor area-level regression models such as ours, it is usual for residuals to exhibit some spatial autocorrelation structure. For certain parts of a country a model will overestimate an outcome given the relationship implied between explanatory and outcome variables; for other parts the outcome will be underestimated. This might occur due to:\n\n* *Spatial dependence \\index{spatial dependence} in variable values* over space. We know that the geography of GB is quite socially distinctive, so it is reasonable to expect, for example, the range in variables like *heavy industry* and *white* to be bounded to economic regions and metropolitan-versus-peripheral regional contexts.\n* *Spatial nonstationarity \\index{spatial nonstationarity} in processes* over space. It is possible that associations between variables might be grouped over space -- that the associations vary for different parts of the country. For example, high levels of *EU-born* migration might affect political attitudes, and thus area-level voting, differently in different parts of the country.\n\nWe can test for and characterise spatial autocorrelation \\index{spatial autocorrelation} in residuals by performing a graphical inference \\index{graphical inference} test, a map line-up [@beecham_map_2017; @wickham_graphical_2010] against a null hypothesis \\index{null hypothesis} of *complete spatial randomness* \\index{complete spatial randomness} (CSR). A plot of real data, the true map of residuals, is hidden amongst a set of decoys; in this case maps with the residual values randomly permuted \\index{permutation} around constituencies. If the real map can be correctly identified from the decoys, then this lends statistical credibility to the claim that the observed data are not consistent with the null of \\index{complete spatial randomness}  CSR. Graphical line-up tests have been used in various domains, also to test regression assumptions [@loy_model_2017]. The map line-up in @fig-lineup \\index{line-up plots} demonstrates that there *is* very obviously spatial and *regional* autocorrelation in residuals, and therefore structure that our regression model misses.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Map line-up of residuals in which the ‘real’ dataset is presented alongside 8 decoy plots generated under assumption of CSR.](figs/06/lineups_hex_annotate.png){#fig-lineup width=100%}\n:::\n:::\n\n\n\n\nThere are different ways of updating our model according to this geographic context. We have talked about patterning in residuals as being *spatial*, with values varying smoothly and continuously depending on location. This might be the case, but given the phenomena we are studying, it also plausible that distinct contexts are linked to regions. The residuals in @fig-lineup -- the real being plot 3 -- do seem to be grouped by regional boundaries, particularly Scotland looks categorically different. This suggests that geographic context might be usefully represented as a *category* rather than a continuous variable (location in *x,y*). We will therefore update our model representing geographic context as a *regional* grouping and cover approaches both to modelling *spatial dependence* in *values* and *spatial nonstationarity* in *processes*. \\index{spatial nonstationarity} \\index{spatial dependence}\n\n### Geographic context as grouped nuisance term\n\nA common approach to treating geographic dependence in the values of variables is to model geographic context as a Fixed Effect \\index{regression modelling!fixed effect|(} (FE). A dummy variable \\index{dummy variable} is created for each group (region in our case), and every region receives a constant.  Any group-level sources of variation \\index{group-level variation} in the outcome are collapsed into the FE variable, which means that regression coefficients are not complicated by this more messy variation -- they now capture the association between demographics and Leave after adjusting for systematic differences in the Leave vote due to region. So, for example, we know that Scotland is politically different from the rest of GB and that this appears to drag down the observed Leave vote for its constituencies. The constant term \\index{regression modelling!constant term} on region adjusts for this and prevents the estimated regression coefficients (inferred associations between variables) from being affected. Also estimated via the constant is the 'base level' in the outcome for each element of the group -- net of demographic composition, the expected Leave vote in each region.\n\nThe linear regression model, extended with the FE term (${\\gamma_{j}}$), for a single variable model:\n\n\\begin{align*}\n       y_{i}&= {\\gamma_{j}}  + \\beta_{1}x_{i1} + \\varepsilon_{i}  \\\\\n\\end{align*}\n\nSo we now estimate the Leave vote in each constituency ($y_{i}$) as a function of:\n\n* ${\\gamma_{j}}$, a constant term similar to an intercept for region $j$, $+$\n* $\\beta_{1}=\\beta_{1}x_{i1}$, the slope, indicating in which direction and to what extent some explanatory variable measured at constituency $i$ is associated with Leave,  $+$\n* $\\varepsilon_{i}$, the difference between $y_{i}$ (the observed value) at constituency $i$ and the *unobservable* true population value of the Leave vote in that constituency (statistical error)\n\n\n\n\n\n\n\n\n\n\nPresented in @fig-outputs-fe are updated regression coefficients \\index{regression modelling!coefficients|(} for a multivariate model fit with a FE on region. In the left panel are the FE constants. Together these capture the variance in Leave vote between regions \\index{regions|(} after accounting for demographic composition. London is of particular interest. When initially analysing variation in the vote, constituencies in Scotland and London were distinctive in voting in much smaller proportions than the rest of the country for Leave. Given the associations we observe with Leave voting and demographic composition, however, if we were to randomly sample two constituencies that contain the same demographic characteristics, one in London and one in another region (say North West), on average we would expect the Leave vote for the London constituency to be higher (~60%) than that sampled from North West (~51%).  A separate and more anticipated pattern is that Scotland would have a lower Leave vote (~38%) -- that is, net of demographics there is some additional context in Scotland that means Leave is lower than in other regions.\n\nIn the right panel are the regression coefficients net of this between-region variation.  Previously the *white* variable had a slight negative association with Leave, counterintuitively. Now the *white* variable has a direction of effect that conforms to expectation -- net of variation in other demographics, increased proportions of *white* residents is associated with increased Leave voting. For another variable, *EU born*, the coefficient still unexpectedly suggests a positive association with Leave.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Output from a multiple regression model of Leave voting against the demographic composition of constituencies, with a Fixed Effect term on region.](figs/06/outputs_fe.png){#fig-outputs-fe width=100%}\n:::\n:::\n\n\n\n\n\n\n### Geographic context as grouped effects\n\n<!-- The benefit of the FE adjustment is that it provides coefficient estimates that are not affected by between-region variation. The FE constants themselves also allow group differences to be quantified net of differences in demographics. However, they simply identify the fact that this variation exists -- they do not permit non-stationarity in *process*. It is conceivable that the strength and direction of association between Leave and the candidate demographic variables may vary between regions. For example, that increased levels of *EU-born* (non-UK) residents might affect area-level voting differently in certain regions than others. -->\n\nRather than simply allowing a constant term to vary, we can update the linear regression model with an \\index{regression modelling!interaction term|(} interaction term (${\\beta_{1j}}{x_{i1}}$) that permits the coefficient estimates to vary depending on region. This means we get a separate constant term and coefficient estimate of the effect of each variable on Leave for every region.\n\n\\begin{align*}\n       y_{i}&= {\\gamma_{j}} + {\\beta_{1j}}x_{i1} + \\varepsilon_{i}  \\\\\n\\end{align*}\n\n* ${\\gamma_{j}}$, a constant term similar to an intercept for region $j$, $+$\n* ${\\beta_{1j}}x_{i1}$, the region-specific slope, indicating in which direction and to what extent some demographic variable at constituency $i$ and in region $j$ is associated with Leave,  $+$\n* $\\varepsilon_{i}$, the difference between $y_{i}$ (the observed value) at constituency $i$ and the *unobservable* true 'population' value of the Leave vote in that constituency (statistical error)\n\nIn @fig-outputs-interact are region-specific coefficients derived from a multivariate model fit with this interaction term. In each region, *degree-educated* has a negative coefficient and with reasonably tight uncertainty estimates, or at least CIs \\index{confidence intervals} that do not cross 0. The other variables are  subject to more uncertainty. The *no-car* variable is also negatively associated with Leave, a variable we thought may separate metropolitan versus peripheral contexts, but the strength of negative association, after controlling for variation in other demographic factors, does vary by region. The *heavy industry* variable, previously identified as being strongly associated with Leave, has a clear positive association only for London and to a much lesser extent for North West and Wales (small coefficients). The *EU-born* variable is again the least consistent as it flips between positive and negative association when analysed at the regional-level: after controlling for variation in other demographic characteristics, it is positively associated with Leave for North West, Scotland, South West, but negatively associated with Leave for the North East, though with coefficients that are subject to much variation.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Output from multiple regression model of Leave vote by demographic composition of constituency with a Fixed Effect and interaction term on region.](figs/06/outputs_interact.png){#fig-outputs-interact width=100%}\n:::\n:::\n\n\n\n\n\n \\index{regression modelling!coefficients|)} \\index{regions|)}  \\index{regression modelling!assumptions|)}\n\n### Estimate volatility and alternative modelling approaches\n\nOur treatment of regression frameworks has in this chapter been reasonably breezy; there are problems that we have not discussed. Introducing FE and interaction terms without adding data reduces statistical power as data are heavily partitioned. Given the fact that our data are hierarchically structured (constituencies sit within regions), hierarchical or multi-level modelling may be more appropriate to this sort of regional grouping. Multi-level modelling \\index{multilevel modelling} uses  \\index{partial pooling} partial pooling, borrowing data to make estimated coefficients more conservative, less locally biased, where there are comparatively few observations in particular groupings [see @gelman_data_2006]. There are also many ways in which associations between values can be modelled *continuously* over space. For the case of geographically weighted regression (GWR) \\index{geographically weighted regression} [@brunsdon_geographically_2002], local regression coefficients for each spatial unit. Geographically Weighted-statistics enable spatial non-stationarity in process to be flexibly explored and characterised -- in this case study, interesting and explainable directions of effect between Leave voting and *EU-born* [see @beecham_locally-varying_2018]. Since GWR involves generating many hundreds of parameter estimates, visual approaches are as ever primarily used in their interpretation and analysis [see @dykes_geographically_2007].\n\n\\index{regression modelling!fixed effect|)} \\index{regression modelling!interaction term|)}\n\n## Techniques\n\nThe technical element to this chapter demonstrates how linear regression models can be specified in R, including approaches to extract model summaries and diagnostics, and of course how to represent and evaluate them using data graphics. Data recording estimated vote shares for Leave by \\index{parliamentary constituency} Parliamentary Constituency, as well as constituency-level Census demographics, were originally collected from the `parlitools` \\index{packages!\\texttt{parlitools}} package.\n\n\n### Import, transform, explore\n\n* Download the `06-template.qmd`[^06-template] file for this chapter, and save it to your `vis4sds` project.\n* Open your `vis4sds` project in RStudio, and load the template file by clicking `File` > `Open File ...` > `06-template.qmd`.\n\n[^06-template]: `https://vis4sds.github.io/vis4sds/files/06-template.qmd`\n\nThe template file lists the required packages: `tidyverse`, `sf` and `tidymodels` \\index{packages!\\texttt{tidymodels}} \\index{packages!\\texttt{simple features}} for extracting model outputs. The processed data with selected 2011 Census demographics can be loaded from the book's accompanying data repository. In this folder is also a `.geojson` file containing a hexagon cartogram of UK parliamentary constituencies, derived from Open-Innovations' `HexJSON` \\index{Open-Innovations \\texttt{HexJSON}}  format.\n\n\\index{z-score transformation|(} Explanatory variables describing the demographic composition of constituencies are recorded as proportions. In order to support comparison in the multivariate models, they must be z-score transformed. The distance between observed values for each 2011 Census \\index{datasets!Census} variable is expressed in standard deviation units from the mean across constituencies for that variable. Our approach is to perform this transformation on each explanatory variable before piping into the model specification. This is achieved with \\index{code!functional programming|(} `across()`. The first argument is the set of columns to which you would like the same function to be applied, and the second is the function you would like to apply. Remembering that `mutate()` works over columns of a data frame, and that a single column of a data frame is a vector of values, the notation `.x` is used to access each element of the columns being worked across. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# z-score transform explanatory variables before model\n# specification.\ncons_data |>\n  mutate(\n    across(\n      .cols=c(younger:heavy_industry),\n      .fns=~(.x-mean(.x))/sd(.x)\n      )\n  )\n  <some-model-specification-code>\n```\n:::\n\n\n\n\n\\index{z-score transformation|)}\n\n\\index{multivariate plots!parallel coordinate plots|(}\n\nIn @fig-scatters and @fig-pcps associations between candidate explanatory variables and Leave are explored using scatterplots and parallel coordinate plots respectively. To avoid cluttering this section, documented code for reproducing these plots is in the `06-template.qmd` file for this chapter and inserted below, but without detailed explanation.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Expose code for parallel coordinate plots\"}\n# Data staging and ggplot2 code for PCPs --------------------------\n\n# Pull out and order variable names on their correlation with Leave.\norder_vars <- cons_data |>\n  mutate(across(c(younger:heavy_industry), ~(.x-mean(.x))/sd(.x))) |> \n  pivot_longer(\n    cols=younger:heavy_industry, names_to=\"expl_var\", values_to=\"prop\"\n    ) |> \n  group_by(expl_var) |>  \n  summarise(cor=cor(leave,prop)) |> ungroup() |> arrange(cor) |>  \n  pull(expl_var)\n# Create staged dataset for plotting.\nplot_data <- cons_data |> \n  mutate(\n    majority=if_else(leave>.5, \"Leave\", \"Remain\"),\n    across(c(leave, younger:heavy_industry), ~(.x-mean(.x))/sd(.x)),\n    decile=ntile(leave, 10),\n    is_extreme = decile > 9 | decile < 2\n  )  |> \n  # Select out variables needed for plot.\n  select(\n    majority, is_extreme, constituency_name, leave, \n    degree, professional, younger, eu_born, no_car, white, own_home, \n    christian, not_good_health, heavy_industry\n    ) |>  \n  # Change polarity in selected variables.\n  mutate(\n    degree=-degree, professional=-professional, younger=-younger, \n    eu_born=-eu_born, no_car=-no_car\n  ) |>  \n  # Gather explanatory variables for along rows.\n  pivot_longer(\n    cols= c(leave:not_good_health), names_to=\"var\", values_to=\"z_score\"\n    ) |> \n  # Recode new explanatory variable as factor ordered according to \n  # known assocs. Reverse order here as coord_flip() used in plot.\n  mutate(\n    var=factor(var, levels=c(\"leave\", order_vars)),\n    var=fct_rev(var)\n  ) \n# Plot PCP.\nplot_data |>  \n  ggplot(\n    aes(x=var, y=z_score, group=c(constituency_name), \n    colour=majority)) +\n  geom_path( alpha=0.15, linewidth=.2) +\n  scale_colour_manual(values=c(\"#2166ac\", \"#b2182b\")) +\n  coord_flip()\n```\n:::\n\n\n\n\n\n::: {.callout-tip icon=\"false\"}\n## Task\n| While the template provides code for reproducing the faceted scatterplots and parallel coordinate plots, there are some omissions. You will notice that in @fig-pcps two very high Leave and Remain constituencies are highlighted, using thicker red and blue lines and labelling.\n| \n| Can you update the ggplot2 spec to create a similar effect? There are different ways of doing this, but you may want to add a separate `geom_segment()` layer `filter()`-ed on these selected boroughs. The text annotations may be generated manually using `annotate()`, or derived from data using `geom_text()`.\n\n:::\n\n\\index{multivariate plots!parallel coordinate plots|)}\n\n\n\n### Model tidily\n\n\\index{packages!\\texttt{tidymodels}|(} \n\nThe most straightforward way of specifying a linear regression model is with the `lm()` function and `summary()` to extract regression coefficients.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- cons_data |>\n  mutate(\n    across(\n      .cols=c(younger:heavy_industry),\n      .fns=~(.x-mean(.x))/sd(.x)\n      )\n  ) %>%\n  lm(leave ~ degree, data=.)\n\nsummary(model)\n# Call:\n# lm(formula = leave ~ degree, data = .)\n#\n# Residuals:\n#      Min       1Q   Median       3Q      Max\n# -0.25521 -0.02548  0.01957  0.05143  0.11237\n#\n# Coefficients:\n#             Estimate Std. Error t value Pr(>|t|)\n# (Intercept)  0.520583   0.002896  179.78   <2e-16 ***\n# degree      -0.088276   0.002898  -30.46   <2e-16 ***\n# ---\n# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n#\n# Residual standard error: 0.07279 on 630 degrees of freedom\n# Multiple R-squared:  0.5956,\tAdjusted R-squared:  0.595\n# F-statistic: 927.9 on 1 and 630 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n\n\nWith `tidymodels`, specifically the `broom` package, we can extract model outputs in a format that adheres to tidy data \\index{tidy data}[@wickham_tidy_2014]. \n\n* `tidy()` returns estimated coefficients as a data frame.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(model)\n# # A tibble: 2 × 5\n# term        estimate std.error statistic   p.value\n# <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n# 1 (Intercept)   0.521    0.00290     180.  0\n# 2 degree       -0.0883   0.00290     -30.5 5.67e-126\n```\n:::\n\n\n\n\n\n* `glance()` returns a single row containing summaries of model fit.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(model)\n# # A tibble: 1 × 12\n# r.sq adj.r.sq  sigma   stat p.value     df logLik   AIC   BIC\n# <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl> <dbl>  <dbl>\n#  1    0.596  0.595 0.0728   928.5.67e-126   1   760 -1514. -1501.\n# # ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n\n\n\n\n* `augment()` returns a data frame of residuals and predictions (fitted values) for the model realisation.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(model)\n# # A tibble: 632 × 8\n# leave  degree .fitted   .resid    .hat .sigma   .cooksd .std.resid\n# <dbl>   <dbl>   <dbl>    <dbl>   <dbl>  <dbl>     <dbl>      <dbl>\n# 1 0.579 -0.211    0.539  0.0398  0.00165 0.0728 0.000247     0.547\n# 2 0.678 -0.748    0.587  0.0914  0.00247 0.0728 0.00195      1.26\n# 3 0.386  1.63     0.376  0.00957 0.00582 0.0729 0.0000509    0.132\n# 4 0.653 -0.964    0.606  0.0473  0.00306 0.0728 0.000648     0.650\n# ...\n```\n:::\n\n\n\n\nThe advantage of generating model diagnostics and outputs that are tidy is that it eases the process of working with many model realisations. This is a common requirement for modern data analysis, where statistical inferences are made empirically from resampling. For example, we may wish to generate single-variable linear regression models separately for each selected explanatory variable. We could use these outputs to annotate the scatterplots in @fig-scatters by their regression line and colour observations according to their residual values, distance from the regression line. These models can be generated with reasonably little code by making use of the package `broom` \\index{packages!\\texttt{broom}|(} and a style of functional programming in R, which is supported by the `purrr` \\index{packages!\\texttt{purrr}|(} package.\n\nExample code:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsingle_model_fits <- cons_data |>\n  mutate(across(c(younger:heavy_industry), ~(.x-mean(.x))/sd(.x))) |>\n  pivot_longer(\n    cols=younger:heavy_industry,\n    names_to=\"expl_var\", values_to=\"z_score\"\n    ) |>\n  # Nest to generate list-column by expl_var.\n  nest(data=-expl_var) |>\n  mutate(\n    # Use map() to iterate over the list of datasets.\n    model = map(data, ~lm(leave ~ z_score, data = .x)),\n    # glance() for each model fit.\n    fits = map(model, glance),\n    # tidy() for coefficients.\n    coefs = map(model, tidy),\n    # augment() for predictions/residuals.\n    values=map(model, augment),\n  )\n\nsingle_model_fits |>\n  # Unnest output from glance.\n  unnest(cols = fits) |>\n  # Remove other list-columns.\n  select(-c(data, model))\n\n# # A tibble: 10 × 15\n# expl_var  r.squared adj.r.sq  sigma  stat   p.value  df logLik   AIC\n# <chr>         <dbl>    <dbl>  <dbl> <dbl>     <dbl> <dbl> <dbl> <dbl>\n# 1 younger    0.289     0.288 0.0965  257. 1.05e- 48   1   582. -1158.\n# 2 own_home   0.185     0.184 0.103   143. 7.42e- 30   1   539. -1071.\n# 3 no_car     0.157     0.155 0.105   117. 3.81e- 25   1   528. -1050.\n# 4 white      0.169     0.168 0.104   128. 3.79e- 27   1   532. -1059.\n# 5 eu_born    0.233     0.232 0.100   191. 3.42e- 38   1   558. -1110.\n# 6 christian  0.238     0.236 0.100   196. 4.95e- 39   1   560. -1114.\n# 7 professi…  0.320     0.319 0.0944  296. 1.08e- 54   1   596. -1186.\n# 8 degree     0.596     0.595 0.0728  928. 5.67e-126   1   760. -1514.\n# 9 not_good…  0.316     0.315 0.0947  291. 5.93e- 54   1   594. -1182.\n# 10 heavy_in… 0.504     0.503 0.0806  640. 5.43e- 98   1   696. -1385.\n# # ℹ 6 more variables: BIC <dbl>, deviance <dbl>, df.residual <int>, \n# #    nobs <int>, coefs <list>, values <list>\n```\n:::\n\n\n\n\n\nCode description:\n\n1. *Setup*: In order to generate separate models for separate explanatory variables, we need to generate nested data frames. These are data frames stored in a special type of column (a `list-column`) in which the values of the column is a list of data frames -- one for each explanatory variable over which we would like to compute a model. You can think of parameterising `nest()` in a similar way to `group_by()`. We first `pivot_longer()` \\index{packages!\\texttt{tidyr}} to generate a data frame where each observation contains the recorded Leave vote for a constituency and its corresponding `z_score` \\index{z-score transformation} value for each explanatory variable.  There are 10 explanatory variables and so `nest()` returns a data frame with the dimensions `10x2` -- a variable identifying the explanatory variable on which the model is to be built (`expl_var`) and a `list-column`, each element containing a data frame  with the dimensions `632x13`.\n2. *Build model*: In `mutate()`, `purrr`'s `map()` function is used to iterate over the list of datasets and fit a model to each nested dataset. The new column `model` is a `list-column` this time containing a list of model objects.\n3. *Generate outputs*: Next, the different cuts of model outputs can be made using `glance(), tidy(), augment()`, with `map()` to iterate over the list of model objects. The new columns are now `list-columns` of data frames containing model outputs.\n4. *Extract outputs*: Finally we want to extract the values from these nested data. This can be achieved using `unnest()` and supplying to the `cols` argument the names of the `list-columns` from which we want to extract values.\n\n### Plot models tidily\n\nIn @fig-outputs estimated regression coefficients are plotted from a multivariate model, annotated with 95% Confidence Intervals. The ggplot2 specification is reasonably straightforward.\n\nThe code for @fig-outputs:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- cons_data |>\n  mutate(across(c(younger:heavy_industry), ~(.x-mean(.x))/sd(.x))) %>%\n  lm(leave ~ degree + eu_born + white  + no_car + not_good_health + \n    heavy_industry, data=.)\n\ntidy(model) |>\n  filter(term != \"(Intercept)\") |>\n  ggplot(\n    aes(x=reorder(term, -estimate),\n        y=estimate, ymin=estimate-1.96*std.error,\n        ymax=estimate+1.96*std.error)\n        ) +\n  geom_pointrange() +\n  coord_flip()\n```\n:::\n\n\n\n\nThe plot specification:\n\n1. *Data*: A data frame of model coefficients extracted from the multivariate model object (`model`) using `tidy()`.\n2. *Encoding*: y-position varies according to the size of the coefficient `estimate` and the 95% confidence intervals, derived from `std.error` and encoded using `ymin` and `ymax` parameters.\n3. *Marks*: `geom_pointrange()`, which understands `ymin` and `ymax`, for the dots with confidence intervals.\n4. *Setting*: `coord_flip()` to make variable names easier to read.\n\n\n### Extend model terms\n\nTo include a Fixed Effect (FE) \\index{regression modelling!fixed effect|(} term on region, the `region` variable is simply added as a variable to `lm()`. However, we must convert it to a factor variable; this has the effect of creating dummies on each value of `region`. Default behaviour within `lm()` is to hold back a reference value of region with FE regression coefficients describing the effect on the outcome of a constituency located in a given region relative to that reference region.  So the reference region (intercept) in the model below is East Midlands -- the first in the factor to appear alphabetically. The signed coefficient estimates for regions identifies whether, after controlling for variation in demographics, the Leave vote for a particular region is expected to be higher or lower than this.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncons_data |>\n  mutate(\n    across(c(younger:heavy_industry), ~(.x-mean(.x))/sd(.x)),\n    region=factor(region)) %>%\n  lm(leave ~ region + degree + eu_born + white  + no_car + \n    not_good_health + heavy_industry, data=.) |>\n  tidy()\n\n# # A tibble: 17 × 5\n# term                      estimate std.error statistic  p.value\n# <chr>                        <dbl>     <dbl>     <dbl>    <dbl>\n#   1 (Intercept)                0.530     0.00581    91.3   0\n# 2 regionEast of England      0.00363   0.00787     0.462 6.45e- 1\n# 3 regionLondon               0.0654    0.00948     6.90  1.30e-11\n# 4 regionNorth East           0.00482   0.00945     0.510 6.10e- 1\n# 5 regionNorth West          -0.0200    0.00728    -2.75  6.12e- 3\n# 6 regionScotland            -0.145     0.00843   -17.2   1.28e-54\n# 7 regionSouth East           0.00377   0.00752     0.502 6.16e- 1\n# 8 regionSouth West          -0.0233    0.00789    -2.95  3.26e- 3\n# 9 regionWales               -0.0547    0.00860    -6.36  3.87e-10\n# 10 regionWest Midlands       0.0236    0.00745     3.17  1.59e- 3\n# 11 regionYorkshire           0.0112    0.00762     1.47  1.41e- 1\n# 12 degree                   -0.0772    0.00339   -22.8   1.30e-83\n# 13 eu_born                   0.0163    0.00308     5.29  1.72e- 7\n# 14 white                     0.0303    0.00314     9.66  1.18e-20\n# 15 no_car                   -0.0336    0.00292   -11.5   6.50e-28\n# 16 not_good_health           0.0102    0.00331     3.07  2.24e- 3\n# 17 heavy_industry            0.0132    0.00266     4.96  9.23e- 7\n```\n:::\n\n\n\n\nWe want our model to represent a dummy for every region, and so we add `-1` to the specification. Doing this removes the intercept or reference region, making $R^2$ no longer meaningful.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncons_data |>\n  mutate(\n    across(c(younger:heavy_industry), ~(.x-mean(.x))/sd(.x)),\n    region=factor(region)) %>%\n  lm(leave ~ region + degree + eu_born + white  + no_car + \n    not_good_health + heavy_industry -1, data=.) |>\n glance()\n\n# # A tibble: 1 × 12\n# r.squared adj.r.squared sigma statistic p.value df logLik AIC \n# <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>\n#   1    0.995  0.995 0.0371  7625.   0     17  1193. -2351. \n# # ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n\n\n\nTo include an Interaction on region, we need to set a variable that will be used to represent these regional constants (`cons`), and the Interaction is added with the notation `:`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- cons_data |>\n  mutate(\n    across(c(younger:heavy_industry), ~(.x-mean(.x))/sd(.x)),\n    region=as.factor(region), cons=1) %>%\n  lm(leave ~ 0 +\n       (cons + degree  + eu_born + white  + no_car + not_good_health +\n       heavy_industry):(region),\n     data=.\n     )\n```\n:::\n\n\n\n\nThe model updated with the regional Interaction term results in many more coefficients that are, as discussed, somewhat unstable. To plot them, as in @fig-outputs-interact, we minimally update the code used to generate the previous model outputs.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(model) |>\n  separate(term, into= c(\"term\", \"region\"), sep=\":\") |>\n  mutate(region=str_remove(region,\"region\")) |>\n  filter(term!=\"cons\") |>\n  ggplot() +\n  geom_col(aes(x=reorder(term, -estimate), y=estimate), alpha=.3)+\n  geom_pointrange(aes(\n    x=reorder(term, -estimate),y=estimate,\n    ymin=estimate-1.96*std.error, ymax=estimate+1.96*std.error\n  )) +\n  geom_hline(yintercept = 0, size=.2)+\n  facet_wrap(~region) +\n  coord_flip()\n```\n:::\n\n\n\n\nThe plot specification:\n\n1. *Data*: A data frame of model coefficients extracted from the multivariate model object using \\index{tidy data} `tidy()`. To make clean plot labels we need to remove unnecessary text in the `term` variable (e.g. \"cons:regionEast Midlands\"). `separate()` allows us to split this column on `:` and then `str_remove()` is quite obvious. We do not wish to plot the FE constants and so `filter()` them out.\n2. *Encoding*: y-position varies according to the size of the coefficient estimate and the 95% confidence intervals, in exactly the same way as for @fig-outputs.\n3. *Marks*: `geom_pointrange()`, encoded as in @fig-outputs. The only addition is light bars in the background (`geom_col()`). This seems to aid interpretation of the direction and size of the coefficients.\n4. *Facets*: `facet_wrap()` on `region` in order to display coefficients estimated separately for each region.\n\n\n\\index{regression modelling!fixed effect|)}\n\n### Evaluate models with lineups\n\n\\index{graphical inference} \\index{line-up plots}\nIn @fig-lineup-fe is a map line-up of the residuals from FE-updated model -- our expectation is that these residuals should *no longer* be spatially autocorrelated, since we collapse regional varation into our FE term. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Map line-up of residuals from model with Fixed Effect on region. The ‘real’ dataset is presented alongside 8 decoy plots generated by randomly permuting the observed residuals around constituencies. Adding the FE term has addressed some of the systematic over- and under-estimation of the vote between regions (compare for example @fig-lineup). There is nevertheless obvious spatial autocorrelation, plot 2 being the real data. Further analysis, for example of the constituencies for which Leave is particularly over- and under-represented, may be instructive.](figs/06/lineups_fe.png){#fig-lineup-fe width=100%}\n:::\n:::\n\n\n\n\nUsing functional-style programming, and index{packages!\\texttt{tidymodels}} `tidymodels`, plot lineups can be generated with surprisingly paired-back code. First generate a model object and extract residuals from it, again making use of `nest()`, `map()` and `augment()`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- cons_data |>\n  select(-c(population, population_density)) |>\n  mutate(\n    across(c(younger:heavy_industry), ~(.x-mean(.x))/sd(.x)),\n    type=\"full_dataset\", region=as.factor(region)\n    ) |>\n  nest(data=-type) |>\n  mutate(\n    # Include `-1` to eliminate the constant term and include \n    # a dummy for every area.\n    model=map(data,\n      ~lm(leave ~ region +  degree  + eu_born + white  + no_car + \n      not_good_health + heavy_industry -1, data=.x)\n      ),\n    # augment() for predictions / residuals.\n    values=map(model, augment)\n  )\n```\n:::\n\n\n\n\nNext, generate permuted \\index{permutation} data by randomly shuffling residual values around constituencies. To do this requires some knowledge of the `rsample` package \\index{packages!\\texttt{rsample}} and its functions. We extract residuals from the `list-column` \\index{code!\\texttt{list-column}} named `values`, remove redundant `list-columns` (with `select()`) and then `unnest()` on the original data and the new `resids` field to return to a dataset where each row is a constituency, but now containing residual values for the multivariate model. From here, we use the `permutations()` function from `rsample`  to shuffle the constituency ID column (`pcon19cd`) randomly around, generating eight permuted datasets and appending the real data (`apparent=TRUE`). This results in a new data frame where each row contains a permuted dataset, stored in a `list-column` named `splits` and labelled via an `id` column. We need to `map()` over `splits` to convert each split object into a data frame, using `rsample`'s `analysis()` function. From here, we `unnest()` to generate a dataset where each row is a constituency and its corresponding residual value (real or shuffled) for a given permutation `id`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npermuted_data <- model |>\n  mutate(\n    resids=map(values, ~.x |>  select(.resid))\n  ) |>\n  select(-c(model, values)) |>\n  unnest(cols=c(data,resids)) |>\n  select(pcon19cd, .resid) |>\n  permutations(permute=c(pcon19cd), times=8, apparent=TRUE) |>\n  mutate(data=map(splits, ~rsample::analysis(.))) |>\n  select(id, data) |>\n  unnest(cols=data)\n```\n:::\n\n\n\n\nNow that we have the permuted dataset, the lineup can be generated straightforwardly with standard ggplot2 code:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Store max value of residuals for setting limits in map colour scheme.\nmax_resid <- max(abs(permuted_data$.resid))\n# Store vector of permutation IDs for shuffling facets in the plots.\nids <- permuted_data |> pull(id) |> unique()\n\ncons_hex |>\n  select(cons_code, region) |>\n  inner_join(permuted_data, by=c(\"cons_code\"=\"pcon19cd\")) |>\n  mutate(id=factor(id, levels=sample(ids))) |>\n  ggplot() +\n  geom_sf(aes(fill=.resid), colour=\"#636363\", linewidth=0.05)+\n  geom_sf(\n    data=. %>% group_by(region) %>% summarise(),\n    colour=\"#636363\", linewidth=0.2, fill=\"transparent\"\n    )+\n  facet_wrap(~id, ncol=3) +\n  scale_fill_distiller(palette=\"RdBu\", direction=1,\n                       limits=c(-max_resid, max_resid), guide=\"none\")\n```\n:::\n\n\n\n\nThe plot specification:\n\n1. *Data*: `inner_join` the permuted data on the simple features file containing the hexagon cartogram boundaries (`cons_hex`). To generate the lineup we `facet_wrap()` on the permutation `id`. By default ggplot2 will draw facets in a particular order -- determined either by the numeric or alphabetical order of the facet variable's values, or by an order determined by a factor variable. Each time we plot the lineups, we want the order in which the real and decoy plots are drawn to vary. Therefore we convert `id` to a factor variable and shuffle the levels (the ordering) around, using the `sample()` function on a vector of permutation IDs (`ids`) before piping to `ggplot()`. Note that we also record the maximum absolute value of the residuals to ensure that they are coloured symmetrically on $0$ (`max_resid`). Finally, you may notice there are two `geom_sf()` calls in the plot specification. The second draws regional boundary outlines across each plot. This is achieved by collapsing the hexagon data on region (using `group_by()` and `summarise()`).\n2. *Encoding*: hexagons are filled according to the residual values (`fill=.resid`).\n3. *Marks*: `geom_sf()` for drawing the hexagon outlines. The first `geom_sf` colours each constituency on its residual value. The second does not encode any data values -- notice there is no `aes()` -- and is simply used to draw the region outlines.\n4. *Facets*: `facet_wrap()` on `region` in order to display coefficients estimated separately for each region.\n5. *Scale*: `scale_fill_distiller()` for  ColorBrewer [@harrower_colorbrewerorg_2003] scheme, using the RdBu palette and with `limits` set to `max_resid`.\n6. *Setting*: The `linewidth` parameter of the hexagon outlines is varied so that the regional outlines in the second call to `geom_sf()` appear more salient. Also here, a transparent `fill` to ensure that the regional outlines do not occlude the encoded residuals.\n\n\\index{Brexit|)} \\index{regression modelling|)} \\index{code!functional programming|)} \\index{packages!\\texttt{broom}|)} \\index{packages!\\texttt{purrr}|)}\n\n## Conclusion\n\nThis chapter demonstrated how visual and computational approaches can be used together in a somewhat more 'traditional' area-level regression analysis.  Associations between constituency-level Leave voting in the UK’s 2016 EU Referendum and selected variables describing the demographic and socio-economic composition of constituencies were explored, with data graphics used to characterise bias in the generated models -- to identify geographic and regional groupings that our early models ignore. Two classes of model update for addressing this geographic grouping were covered: those that treat geographic dependence \\index{spatial dependence} in the values of variables as a nuisance term that is to be quantified and controlled away, and those that explicitly try to model for geographic grouping \\index{spatial nonstationarity} in processes. We introduced some initial techniques for dealing with both, treating geography as a categorical variable: a Fixed Effect term to assess regional dependence and Interaction term to assess regional non-stationarity. Importantly, the chapter reused some of the `dplyr` and functional programming code templates instrumental for working over models. There was a step-up in code complexity. Hopefully you will see in the next chapter that this sort of functional programming style [@wickham_r_2023] \\index{code!functional programming} greatly aids the process of performing and working with resampled datasets, a key feature of modern computational data analysis.\n\n\\index{datasets!EU referendum|)} \\index{packages!\\texttt{tidymodels}|)}\n\n## Further Reading\n\nAn area-level analysis of the Brexit vote:\n\n* Beecham, R., Williams, N. and Comber, L. 2020. “Regionally-structured explanations behind area-level populism: An update to recent ecological analyses.” *PLOS One*, 15(3): e0229974. doi: 10.1371/journal.pone.0229974.\n\nOn modelling for geographic dependence and non-stationarity:\n\n* Comber, A., Brunsdon, C., Charlton, M. et al. 2023. \"A route map for successful applications of Geographically Weighted Regression.\" *Geographical Analysis*, 55 (1): 155--178. doi: 10.1111/gean.12316.\n\n\n* Wolf, L. J. et al., 2023. “On Spatial and Platial Dependence: Examining Shrinkage in Spatially Dependent Multilevel Models.” *Annals of the American Association of Geographers*, 55(1): 1–13. doi: 10.1080/24694452.2020.1841602.\n\nThe original graphical inference paper:\n\n* Buja, A., Cook, D., Hofmann, H., Lawrence, M., Lee, E.K., Swayne, D. F. and  Wickham, H. 2010. “Statistical Inference for Exploratory Data Analysis and Model Diagnostics.” *Royal Society Philosophical Transactions A*, 367:4361– 83. doi: 10.1098/rsta.2009.0120.\n\nA guide to model building in the `tidyverse`:\n\n* Ismay, C. and Kim, A. 2020. \"Statistical Inference via Data Science: A ModernDive into r and the Tidyverse\", New York, NY: *CRC Press*. doi: 10.1201/9780367409913.\n\n* Kuhn, M. and Silge, J. 2023. \"Tidy Modelling with R.\", Sebastopol, CA: *O'Reilly*.\n\nA quick guide to functional programming in R and `tidyverse`:\n\n* Wickham, H., Çetinkaya-Rundel, M., Grolemund, G. 2023, \"R for Data Science, 2nd Edition\", Sebastopol, CA: *O'Reilly*.\n  - Chapter 27.",
    "supporting": [
      "06-model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"array\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"multirow\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"wrapfig\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"colortbl\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"pdflscape\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabu\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttablex\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"ulem\"]},{\"type\":\"character\",\"attributes\":{},\"value\":[\"normalem\"]},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"makecell\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"xcolor\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"array\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"multirow\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"wrapfig\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"colortbl\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"pdflscape\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabu\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttablex\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"ulem\"]},{\"type\":\"character\",\"attributes\":{},\"value\":[\"normalem\"]},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"makecell\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"xcolor\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}