{
  "hash": "8313d63867159a7391804936b71f1b84",
  "result": {
    "engine": "knitr",
    "markdown": "# Visualization Fundamentals {#sec-visual}\n\n\n\n\n\n```{=html}\n<script defer src=\"/fontawesome.min.js\"></script>\n```\n\n::: {.cell}\n\n:::\n\n\n\n\n\nBy the end of this chapter you should gain the following knowledge and practical skills.\n\n::: {.callout-note icon=\"false\"}\n## Knowledge outcomes\n\n-   [ ] Recognise the characteristics of effective data graphics.\n-   [ ] Understand that there is a *grammar* of graphics, and that this grammar underpins modern visualization toolkits (ggplot2, vega-lite and Tableau).\n-   [ ] Appreciate how visual channels and knowledge of their encoding effectiveness can be used to design and evaluate data graphics.\n:::\n\n::: {.callout-note icon=\"false\"}\n## Skills outcomes\n\n-   [ ] Write ggplot2 code to generate statistical graphics (histograms, bar charts, scatterplots, choropleth maps).\n-   [ ] Update that code to *layer* graphics with multiple variables. \n-   [ ] Write code to manipulate the order and colour of data items in statistical graphics.\n-   [ ] Advanced: create glyphmaps in ggplot2 by writing code that works on shape primitives.\n-   [ ] (Very) Advanced: create dot-density maps in ggplot2 using re-sampling and functional programming. \n:::\n\n## Introduction\n\nThis chapter outlines the fundamentals of visualization design. It offers a position on what effective data graphics should do, before discussing the processes that take place when creating data graphics. A framework -- a vocabulary and grammar -- for supporting this process is presented which, combined with established knowledge on visual perception, helps describe, evaluate and create effective data graphics. Talking about a vocabulary and grammar of data and graphics may sound somewhat abstract. However, through an analysis of 2019 General Election results data, the chapter will demonstrate how these concepts are fundamental to visual data analysis.\n\n\n\n\n\n```{=html}\n<!-- ::: {.callout-tip icon=false}\n## Task\nWatch [Miriah Meyer's](https://www.cs.utah.edu/~miriah/) TEDx talk, *Information Visualization for Scientific Discovery*, which provides an introduction to many of the concepts covered in the chapter.\n::: -->\n```\n\n\n\n\n## Concepts\n\n### Effective data graphics {#sec-effective}\n\n\n\\index{data visualization!definition}\n\nData graphics take numerous forms and are used in many different ways by scientists, journalists, designers and many more. While the intentions of those producing them may vary, data graphics that are effective generally have the following characteristics:\n\n-   Expose complex structure, connections and comparisons that could not be achieved easily via other means;\n-   Are data rich, presenting many numbers in a small space;\n-   Reveal patterns at several levels of detail, from broad overview to fine structure;\n-   Are concise, emphasising dimensions of a dataset without extraneous details;\n-   Generate an aesthetic response, encouraging people to engage with the data or question.\n\n<!-- Given these characteristics  -->\n\\index{Washington Post|(} \\index{elections} \\index{US counties} \\index{maps!glyphmaps|(} \\index{US presidential election|(}\nConsider the data graphic in @fig-wp-map, which presents an analysis of the 2016 US Presidential Election, or the *Peaks and Valleys of Trump and Clinton's Support*. The map is reproduced from an article in The Washington Post [@gamio_how_2016]. Included in the bottom margin is a choropleth map coloured according to party majority, more standard practice for reporting county-level voting. Gamio and Keating's [-@gamio_how_2016] graphic is clearly *data rich*, encoding many more data items than does the standard choropleth. It is not simply the data density that makes the graphic successful, however. There are careful design choices that help *support comparison* and emphasise *complex structure*. By varying the height of triangles according to the number of votes cast, the thickness according to whether or not the result for Trump/Clinton was a landslide and rotating the map 90 degrees, the very obvious differences between metropolitan, densely populated coastal counties that voted emphatically for Clinton and the vast number of suburban, provincial town and rural counties (everywhere else) that voted for Trump, are exposed.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Map of 2016 US presidential election results. Note that for copyright reasons this is a re-implementation in ggplot2 of Gamio and Keating's [-@gamio_how_2016] original, which appeared in The Washington Post.](figs/03/wp_long.png){#fig-wp-map width=100%}\n:::\n:::\n\n\n\n\n\n\\index{Washington Post|)} \\index{maps!glyphmaps|)}\n\n### Grammar of Graphics {#sec-grammar}\n\n\\index{Grammar of Graphics|(}\n\n> [*Data graphics visually display measured quantities by means of the combined use of points, lines, a coordinate system, numbers, symbols, words, shading, and color.*]{.content-visible when-format=\"html\"} [\\enquote{Data graphics visually display measured quantities by means of the combined use of points, lines, a coordinate system, numbers, symbols, words, shading, and color.}]{.content-visible when-format=\"pdf\"}\n>\n> @tufte_visual_1983\n\nSo the Washington Post graphic demonstrates a judicious *mapping* of data to visuals, underpinned by a close appreciation of the analysis context. The act of carefully considering how best to leverage visual systems given the available data and analysis priorities is key to designing effective data graphics. Leland Wilkinson's *Grammar of Graphics* [-@wilkinson_grammar_1999] captures this process of turning data into visuals. Wilkinson's [-@wilkinson_grammar_1999] thesis is that graphics can be described in a consistent way according to their structure and composition. This has obvious benefits for building visualization toolkits. If different chart types and combinations can be reduced to a common vocabulary and grammar, then the process of designing and *generating* graphics of different types can be systematised.\n\n\nWilkinson's [-@wilkinson_grammar_1999] grammar separates the construction of data graphics into a series of components. Below are the components of the *Layered Grammar of Graphics* on which ggplot2 is based [@wickham_layered_2010], adapted from Wilkinson's [-@wilkinson_grammar_1999] original work. The components in @fig-gog are used to assemble  ggplot2 specifications. Those to highlight at this stage are in [[emphasis]{style=\"color:#000000;\"}]{.content-visible when-format=\"html\"} [\\textbf{emphasis}]{.content-visible when-format=\"pdf\"}: the [[data]{style=\"color:#5A91CA;font-weight:bold\"}]{.content-visible when-format=\"html\"} [\\textbf{\\dat{data}}]{.content-visible when-format=\"pdf\"} containing the variables of interest, the [[marks]{style=\"color:#E17637;font-weight:bold\"}]{.content-visible when-format=\"html\"} [\\textbf{\\colmark{marks}}]{.content-visible when-format=\"pdf\"}  used to represent data and the visual [[channels]{style=\"color:#62B743;font-weight:bold\"}]{.content-visible when-format=\"html\"} [\\textbf{\\channels{channels}}]{.content-visible when-format=\"pdf\"} through which variables are encoded.\n\n\\index{Grammar of Graphics|)}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Components of Wickham's [-@wickham_layered_2010] Layered Grammar of Graphics.](figs/03/gog.png){#fig-gog width=100%}\n:::\n:::\n\n\n\n\n\n\n\n\\index{multivariate plots!scatterplots} \\index{elections} \\index{parliamentary constituency}\nTo demonstrate this, let's generate some scatterplots based on the 2019 General Election data. Two variables worth exploring for association here are: `con_1719`, the change in Conservative vote share by constituency between 2017-2019, and `leave_hanretty`, the size of the Leave vote in the 2016 EU referendum, estimated at Parliamentary Constituency level [via @hanretty_areal_2017].\n\nIn @fig-gog-demo are three plots, accompanied by ggplot2 specifications used to generate them. Reading the graphics and the associated code, you should get a feel for how ggplot2 specifications are constructed:\n\n1.  Start with a data frame, in this case 2019 General Election results for UK Parliamentary Constituencies. The data are passed to ggplot2 (`ggplot()`) using the pipe operator (`|>`). Also at this stage, we consider the variables to encode and their measurement [[type]{style=\"color:#5A91CA;font-weight:bold\"}]{.content-visible when-format=\"html\"} [\\textbf{\\dat{type}}]{.content-visible when-format=\"pdf\"} -- both `con_1719` and `leave_hanretty` are `ratio` scale variables.\n2.  Next is the encoding (`mapping=aes()`), which determines how the data are to be mapped to visual [[channels]{style=\"color:#62B743;font-weight:bold\"}]{.content-visible when-format=\"html\"} [\\textbf{\\channels{channels}}]{.content-visible when-format=\"pdf\"}. In a scatterplot, horizontal and vertical position varies in a meaningful way, in response to the values of a dataset. Here the values of `leave_hanretty` are mapped along the x-axis, and the values of `con_1719` are mapped along the y-axis.\n3.  Finally, we represent individual data items with [[marks]{style=\"color:#E17637;font-weight:bold\"}]{.content-visible when-format=\"html\"} [\\textbf{\\colmark{marks}}]{.content-visible when-format=\"pdf\"} using the `geom_point()` geometry.\n\nIn the middle plot, the grammar is updated such that the points are coloured according to `winning_party`, a variable of type categorical `nominal`. In the bottom plot constituencies that flipped from Labour-to-Conservative between 2017-19 are emphasised by varying the `shape` (filled and not filled) and transparency (`alpha`) of points.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Plots, grammars and underlying ggplot2 specifications for the scatterplot.](figs/03/gog_demo_redesign.png){#fig-gog-demo width=100%}\n:::\n:::\n\n\n\n\n\n<!-- ![Plots, grammars and underlying ggplot2 specifications for the scatterplot.](figs/03/gog-demo.png){#fig-gog-demo width=100% fig-align=\"center\"} -->\n\n\n\n\n\n```{=html}\n<!-- ::: {.callout-note}\n## On ggplot2 'specifications'\nIt is understandable if at this stage the specifications in @fig-gog-demo still seem alien to you. We will be updating, expanding and refining ggplot2 specifications throughout the book to support all aspects of modern data analysis: from data cleaning and exploratory analysis through to model evaluation and communication.\n::: -->\n```\n\n\n\n\n### Marks and visual channels\n\n\n\n\n\n```{=html}\n<!-- >  Effective data visualization design is concerned with representing data through marks and visual channels in a way that best conveys the properties of the data that are to be depicted.\n>\n> via [Jo Wood](https://www.gicentre.net/jwo/index) -->\n```\n\n\n\n\n\n\\index{visual channels|(} \\index{data visualization!marks|(}\nIn our descriptions [[marks]{style=\"color:#E17637;font-weight:bold\"}]{.content-visible when-format=\"html\"}  [\\textbf{\\colmark{marks}}]{.content-visible when-format=\"pdf\"} was used as an alternative term for *geometry* and visual encoding [[channels]{style=\"color:#62B743;font-weight:bold\"}]{.content-visible when-format=\"html\"} [\\textbf{\\channels{channels}}]{.content-visible when-format=\"pdf\"}  as an alternative for *aesthetics*. We also paid special attention to the [[data types]{style=\"color:#5A91CA;font-weight:bold\"}]{.content-visible when-format=\"html\"} [\\textbf{\\dat{data types}}]{.content-visible when-format=\"pdf\"}  that were encoded. *Marks* are graphical elements such as *bars*, *lines*, *points* and *ellipses* that can be used to represent data items. In ggplot2 marks are accessed through the function layers prefaced with `geom_*()`. Visual *channels* are attributes such as *colour*, *size* and *position* that, when mapped to data, affect the appearance of marks in response to the values of a dataset. These attributes are controlled via the `aes()` (aesthetics) function in ggplot2.\n\n*Marks* and *channels* are terms used routinely in \\index{information visualization} Information Visualization, an academic discipline devoted to the study of data graphics, and most notably by Tamara @munzner_visualization_2014 in her textbook *Visualization Analysis and Design*. Munzner's [-@munzner_visualization_2014] work synthesises over foundational research in Information Visualization and Cognitive Science testing how effective different visual channels are at supporting specific tasks. @fig-munzner is adapted from @munzner_visualization_2014 and lists the main visual channels with which data might be encoded. The grouping and order of the figure is meaningful. Channels are grouped according to the tasks to which they are best suited and then ordered according to their effectiveness at supporting those tasks. The left grouping displays *magnitude:order* channels -- those that are best suited to tasks aimed at quantifying data items. The right grouping displays *identity:category* channels -- those that are most suited to supporting tasks that involve isolating and associating data items.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Visual channels to which data items can be encoded, adapted from @munzner_visualization_2014.](figs/03/munzner_effect_trans.png){#fig-munzner width=65%}\n:::\n:::\n\n\n\n\n\n\n\\index{visual channels|)} \\index{data visualization!marks|)}\n\n### Evaluating designs\n\n\\index{data visualization!evaluation}\n\nThe effectiveness rankings of visual channels in @fig-munzner are not simply based on Munzner's preference. They are informed by detailed experimental work by @cleveland_graphical_1984, later replicated by @heer_crowdsourcing_2010, which involved conducting controlled experiments testing people's ability to make judgements from graphical elements. We can use @fig-munzner to help make decisions around which data item to encode with which visual channel. This is particularly useful when designing data-rich graphics, where several data items are to be encoded simultaneously. @fig-munzner also offers a low cost way of *evaluating* different designs against their encoding effectiveness.\n\n\\index{Washington Post|(} \\index{maps!glyphmaps|(}\n\nTo illustrate this, we can use Munzner's ranking of channels to evaluate The Washington Post graphic discussed in @fig-wp-map. [[Table 3.2](@tbl-wp-eval-size-tex)]{.content-visible when-format=\"pdf\"}[[Table 3.2](#tab:wp-eval-size)]{.content-visible when-format=\"html\"} provides a summary of the encodings used in the graphic. US counties are represented using a peak-shaped *mark*. The key purpose of the graphic is to depict the geography of voting outcomes. The most effective quantitative channel -- position on an aligned scale -- is used to order the county marks with a geographic arrangement. With the positional channels taken, the two quantitative measures are encoded with the next highest ranked channel, length or 1D size: height varies according to number of *total votes cast* and width according to *margin size*. The marks are additionally encoded with two categorical variables: whether the county-level result was a *landslide* and also the *winning party*. Since the intention is to give greater visual saliency to counties that resulted in a landslide, this is an `ordinal` variable encoded with a quantitative channel: area / 2D size. The *winning party*, a categorical `nominal` variable, is encoded using colour hue.\n\n\n\n\n\n::: {.cell tbl-cap='Encoding effectiveness for Gamio and Keating\\'s (2016) Washington Post graphic that emphasises *vote margin and size* of counties using triangle marks.'}\n\n:::\n\n::: {.cell tbl-cap='Encoding effectiveness for Gamio and Keating\\'s (2016) Washington Post graphic that emphasises *vote margin and size* of counties using triangle marks.'}\n::: {.cell-output-display}\n`````{=html}\n<table>\n<caption>Encoding effectiveness for Washington Post graphic that emphasises *vote margin and size* of counties using triangle marks.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;border-bottom: 1px solid;\"> Data item </th>\n   <th style=\"text-align:left;border-bottom: 1px solid;\"> Type </th>\n   <th style=\"text-align:left;border-bottom: 1px solid;\"> Channel </th>\n   <th style=\"text-align:left;border-bottom: 1px solid;\"> Rank </th>\n  </tr>\n </thead>\n<tbody>\n  <tr grouplength=\"4\"><td colspan=\"4\" style=\"border-bottom: 0px solid;\">Magnitude:Order</td></tr>\n<tr>\n   <td style=\"text-align:left;padding-left: 2em;width: 40%; \" indentlevel=\"1\"> County location </td>\n   <td style=\"text-align:left;width: 20%; font-family:  Monospace\"> interval </td>\n   <td style=\"text-align:left;width: 30%; \"> position in x,y </td>\n   <td style=\"text-align:left;\"> 1. quant </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;width: 40%; \" indentlevel=\"1\"> Total votes cast </td>\n   <td style=\"text-align:left;width: 20%; font-family:  Monospace\"> ratio </td>\n   <td style=\"text-align:left;width: 30%; \"> length </td>\n   <td style=\"text-align:left;\"> 3. quant </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;width: 40%; \" indentlevel=\"1\"> Margin size </td>\n   <td style=\"text-align:left;width: 20%; font-family:  Monospace\"> ratio </td>\n   <td style=\"text-align:left;width: 30%; \"> length </td>\n   <td style=\"text-align:left;\"> 3. quant </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;width: 40%; \" indentlevel=\"1\"> Is landslide </td>\n   <td style=\"text-align:left;width: 20%; font-family:  Monospace\"> ordinal </td>\n   <td style=\"text-align:left;width: 30%; \"> area </td>\n   <td style=\"text-align:left;\"> 5. quant </td>\n  </tr>\n  <tr grouplength=\"1\"><td colspan=\"4\" style=\"border-bottom: 0px solid;\">Identity:Category</td></tr>\n<tr>\n   <td style=\"text-align:left;padding-left: 2em;width: 40%; \" indentlevel=\"1\"> Winning party </td>\n   <td style=\"text-align:left;width: 20%; font-family:  Monospace\"> nominal </td>\n   <td style=\"text-align:left;width: 30%; \"> colour hue </td>\n   <td style=\"text-align:left;\"> 2. cat </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\n\n\nEach of the encoding choices follow conventional wisdom in that data items are encoded using visual channels appropriate to their measurement level. Glancing down the \"rank\" column, the graphic has high effectiveness. While technically *spatial region* is the most effective channel for encoding `nominal` data, it is already in use as the marks are arranged by geographic position. Additionally, it makes sense to distinguish [[Republican]{style=\"color:#DB534D;font-weight:bold\"}]{.content-visible when-format=\"html\"} [\\textbf{\\rep{Republican}}]{.content-visible when-format=\"pdf\"} and [[Democrat]{style=\"color:#3879A1;font-weight:bold\"}]{.content-visible when-format=\"html\"} [\\textbf{\\dem{Democrat}}]{.content-visible when-format=\"pdf\"} wins using the colours with which they are always represented. Given the fact that the positional channels represent geographic *location*, length to represent *votes cast* and *vote margin*, the only superior visual channel to 2D area that could be used to encode the *landslide* variable is *orientation*. There are very good reasons for not varying the orientation of the arrow marks. Most obvious is that this would undermine perception of length encodings used to represent the vote margin (width) and absolute vote size (height).\n\n::: callout-note\n## Visualization design and trade-offs\n\n\\index{data visualization!trade-offs}\nData visualization design almost always involves trade-offs. A general principle is to identify and prioritise data and analysis tasks, then match the most effective encodings to the data and tasks that have the greatest priority. Less important data items and tasks therefore get less effective encodings. In practice, visualization design involves exercising more creative thinking -- it is sometimes preferable to defy convensional wisdom in order to provoke some desired response. Either way, good visualization design is sensitive to this interplay between tasks, data and encoding.\n:::\n\n\n### Symbolisation\n\n\\index{symbolisation|(}\n\n> [*Symbolization is the process of encoding something with meaning in order to represent something else. Effective symbol design requires that the relationship between a symbol and the information that symbol represents (the referent) be clear and easily interpreted.*]{.content-visible when-format=\"html\"} [\\enquote{Symbolization is the process of encoding something with meaning in order to represent something else. Effective symbol design requires that the relationship between a symbol and the information that symbol represents (the referent) be clear and easily interpreted.}]{.content-visible when-format=\"pdf\"}\n>\n> @white_symbolization_2017\n\nImplicit in the discussion above, and when making design decisions, is the importance of *symbolisation*. From the original Washington Post article, the overall pattern that can be discerned is of population-dense coastal and metropolitan counties voting Democrat -- densely-packed, tall, wide and blue [<i style=\"color:#3879A1;font-weight:bold\" class=\"fas fa-lg fa-angle-up\"></i>]{.content-visible when-format=\"html\"} [\\Large\\bm{\\dem{$\\wedge{}$}}]{.content-visible when-format=\"pdf\"} marks -- contrasted with population-sparse rural and small town areas voting Republican -- short, wide and red [<i style=\"color:#DB534D;font-weight:bold\" class=\"fas fa-angle-up\"></i>]{.content-visible when-format=\"html\"} [\\normalsize\\bm{\\rep{$\\wedge{}$}}]{.content-visible when-format=\"pdf\"}  marks. The graphic evokes a distinctive landscape of voting behaviour, emphasised by its caption: \"*The peaks and valleys of Trump and Clinton's support*\".\n\n\\index{Swing}\n*Symbolisation* is used equally well in a variant of the graphic emphasising two-party *Swing* between the 2012 and 2016 elections (@fig-wp-swing). Each county is represented as a [[\\|]{style=\"font-weight:bolder\"}]{.content-visible when-format=\"html\"} [\\textbf{$\\vert{}$}]{.content-visible when-format=\"pdf\"} mark. The *Swing* variable is then encoded by continuously varying mark angles: counties swinging [[Republican]{style=\"color:#DB534D;font-weight:bold\"}]{.content-visible when-format=\"html\"} [\\rep{\\textbf{Republican}}]{.content-visible when-format=\"pdf\"} are angled to the right [[/]{style=\"color:#DB534D;font-weight:bolder\"}]{.content-visible when-format=\"html\"} [\\rep{\\textbf{$\\slash{}$}}]{.content-visible when-format=\"pdf\"}; counties swinging [[Democrat]{style=\"color:#3879A1;font-weight:bold\"}]{.content-visible when-format=\"html\"} [\\dem{\\textbf{Democrat}}]{.content-visible when-format=\"pdf\"} are angled to the left [[\\\\]{style=\"color:#3879A1;font-weight:bolder\"}]{.content-visible when-format=\"html\"} [\\bm{\\dem{$\\backslash{}$}}]{.content-visible when-format=\"pdf\"}. Although *angle* is a less effective channel at encoding quantities than is *length*, there are obvious links to the political phenomena in the symbolisation -- angled right for counties that moved to the right politically. There are further useful properties in this example. Since county voting is spatially auotocorrelated, we quickly assemble from the graphic dominant patterns of Swing to the Republicans (Great Lakes, rural East Coast), predictable Republican stasis (the Midwest) and more isolated, locally exceptional swings to the Democrats (rapidly urbanising counties in the deep South).\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Map of swing in 2016 US presidential election results. Note that for copyright reasons this is a re-implementation in ggplot2 of Gamio and Keating's [-@gamio_how_2016] original, which appeared in The Washington Post.](figs/03/wp_spoke.png){#fig-wp-swing width=100%}\n:::\n:::\n\n\n\n\n\n\\index{symbolisation|)} \\index{Washington Post|)} \\index{maps!glyphmaps|)} \\index{US presidential election|)}\n\n\n::: {.callout-tip icon=\"false\"}\n## Task 1\n\n\\index{visual channels} \\index{data visualization!marks}\n\nComplete the description table below to identify each *data item* encoded in @fig-wp-swing along with its *measurement level*, *visual mark* and *visual channel* and the effectiveness rank of this encoding, according to @munzner_visualization_2014.\n\n| Data item         | Measurement level | Visual mark  | Visual channel | Rank         |\n|-------------------|-------------------|--------------|----------------|--------------|\n| `County location` | `<enter here>`    | `<enter here>` | `<enter here>`| `<enter here>` |\n| `...`             | `...`             | `...`        | `...`          | `...`        |\n| `...`             | `...`             | `...`        | `...`          | `...`        |\n| `...`             | `...`             | `...`        | `...`          | `...`        |\n| `...`             | `...`             | `...`        | `...`          | `...`        |\n| `...`             | `...`             | `...`        | `...`          | `...`        |\n:::\n\n\n### Colour\n\n\\index{colour|(}\n\nColour is a very powerful visual channel. When considering how to encode data with colour, it is helpful to consider three properties:\n\n* *Hue*: what we generally refer to as \"colour\" in everyday life -- red, blue, green.\n* *Saturation*: how much  of a colour there is.\n* *Luminance/Brightness*: how dark or light a colour is. \n\n\nThe ultimate rule is to use these properties of colour in a way that matches the properties of the data (@fig-brewer). Categorical `nominal` data -- data that cannot be easily ordered -- should be encoded using discrete colours with no obvious order; so colour hue. Categorical `ordinal` data -- data whose categories can be ordered -- should be encoded with colours that contain an intrinsic order; saturation or brightness (colour value) allocated into perceptually-spaced gradients. `Quantitative` data -- data that can be ordered and contain values on a continuous scale -- should also be encoded with saturation or brightness, expressed on a continuous scale. As we will discover shortly, these principles are applied by default in ggplot2, along with access to perceptually valid schemes [e.g. @harrower_colorbrewerorg_2003].\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Colour schemes matched to variable measurement level.](figs/03/brewer_colour.png){#fig-brewer width=70%}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n::: callout-note\n## On colour\n\nThere are very many considerations when using colour to support visual data analysis and communication -- more than we have space for in this chapter. Lisa Charotte-Muth's [-@muth_your_2018] *Guide to Colours in Data Visualization*[^03-visual-1] is an excellent outline of the decision-space.\n:::\n\n\\index{colour|)}\n\n[^03-visual-1]: `https://blog.datawrapper.de/colorguide/`\n\n## Techniques {#sec-techniques}\n\nThe technical component to this chapter analyses data from the 2019 UK General Election, reported at Parliamentary Constituency level. After importing and describing the dataset, we will generate data graphics that expose patterns in voting behaviour.\n\n-   Download the `03-template.qmd`[^03-template] file for this chapter and save it to your `vis4sds` project.\n-   Open your `vis4sds` project in RStudio and load the template file by clicking `File` \\> `Open File ...` \\> `03-template.qmd`.\n\n[^03-template]: `https://vis4sds.github.io/vis4sds/files/03-template.qmd`\n\n### Import\n\n\\index{packages!\\texttt{parlitools}} \\index{elections} \\index{datasets!UK General Election|(} \\index{UK General Election|(}\n\nThe template file lists the required packages -- `tidyverse` and `sf` -- and links to the 2019 UK General Election dataset, stored on the book's accompanying data repository. These data were initially collected via the `parlitools` R package, which is no longer maintained.\n\nThe data frame of 2019 UK General Election data is called `bes_2019`. This stores results data released by the House of Commons Library [@uberoi_general_2020]. We can get a quick overview with a call to `glimpse(<dataset-name>)`. `bes_2019` has 650 rows, one for each parliamentary constituency, and 118 columns. In the columns are variables reporting vote numbers and shares for the main political parties for the 2019 and 2017 General Elections, as well as names and codes (`ID`s) for each constituency and the local authority, region and country in which they are contained.\n\n\\index{Swing|(}\nWe will replicate some of the visual data analysis in @beecham_using_2020. For this we need to calculate an additional variable, Butler Swing [@butler_why_1990]: the average change in share of the vote won by two parties contesting successive elections. Code for calculating this variable, named `swing_con_lab`, is in the `03-template.qmd`. The only other dataset to load is a `.geojson` file containing simplified geometries of constituencies, originally from ONS Open Geography Portal. This is a special class of data frame containing a Simple Features [@pebesma_simple_2018] `geometry` column.\n\n\n\n### Summarise\n\nYou may be familiar with the result of the 2019 General Election, a landslide Conservative victory that confounded expectations. To start, we can quickly compute some summary statistics around the vote. In the code below, we count the number of seats won and overall vote share by party. For the vote share calculation, the code is a little more elaborate than we might wish at this stage. We need to reshape the data frame using `pivot_wider()` such that each row represents a vote for a party in a constituency. From here the vote share for each party can be easily computed. \\index{packages!\\texttt{tidyr}} \\index{tidy data}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of constituencies won by party.\nbes_2019 |>\n  group_by(winner_19) |>\n  summarise(count=n()) |>\n  arrange(desc(count))\n## # A tibble: 11 x 2\n##    winner_19                        count\n##    <chr>                            <int>\n##  1 Conservative                       365\n##  2 Labour                             202\n##  3 Scottish National Party             48\n##  4 Liberal Democrat                    11\n##  5 Democratic Unionist Party            8\n##  6 Sinn Fein                            7\n##  7 Plaid Cymru                          4\n##  8 Social Democratic & Labour Party     2\n##  9 Alliance                             1\n## 10 Green                                1\n## 11 Speaker                              1\n\n# Share of vote by party.\nbes_2019 |>\n  # Select cols containing vote counts by party.\n  select(\n    constituency_name, total_vote_19, \n    con_vote_19:alliance_vote_19, region\n    ) |>\n  # Pivot to make each row a vote for a party in a constituency.\n  pivot_longer(\n    cols=con_vote_19:alliance_vote_19, \n    names_to=\"party\", values_to=\"votes\"\n    ) |>\n  # Use some regex to pull out party name.\n  mutate(party=str_extract(party, \"[^_]+\")) |>\n  # Summarise over parties.\n  group_by(party) |>\n  # Calculate vote share for each party.\n  summarise(vote_share=sum(votes, na.rm=TRUE)/sum(total_vote_19)) |>\n  # Arrange parties descending on vote share.\n  arrange(desc(vote_share))\n\n## # A tibble: 12 x 2\n##    party    vote_share\n##    <chr>         <dbl>\n##  1 con         0.436\n##  2 lab         0.321\n##  3 ld          0.115\n##  4 snp         0.0388\n##  5 green       0.0270\n##  6 brexit      0.0201\n##  7 dup         0.00763\n##  8 sf          0.00568\n##  9 pc          0.00479\n## 10 alliance    0.00419\n## 11 sdlp        0.00371\n## 12 uup         0.00291\n```\n:::\n\n\n\n\n\nWhile the Conservative party held 56% of constituencies in 2019 election, they won only 44% of the vote. The equivalent figures for Labour were 31% and 32% respectively. And although the Conservatives gained many more constituencies than they did in 2017 (when they won just 317, 49% of constituencies) their vote share hardly shifted between those elections -- in 2017 the Conservative vote share was 43%. This fact is interesting as it may suggest some movement in where the Conservative party gained its majorities in 2019.\n\nBelow are some summary statistics computed over the newly created `swing_con_lab` variable. As the Conservative and Labour votes are negligible in Northern Ireland, it makes sense to focus on Great Britain for our analysis of Conservative-Labour Swing, and so the first step in the code is to create a new data frame filtering out Northern Ireland.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_gb <- bes_2019 |>\n  filter(region != \"Northern Ireland\") |>\n  # Also recode to 0 Chorley and Buckingham, incoming/outgoing speaker.\n  mutate(\n    swing_con_lab=if_else(\n      constituency_name %in% c(\"Chorley\", \"Buckingham\"), 0,\n      0.5*((con_19-con_17)-(lab_19-lab_17))\n      )\n  )\n\ndata_gb |>\n  summarise(\n    min_swing=min(swing_con_lab),\n    max_swing=max(swing_con_lab),\n    median_swing=median(swing_con_lab),\n    num_swing=sum(swing_con_lab>0),\n    num_landslide_con=sum(con_19>50, na.rm=TRUE),\n    num_landslide_lab=sum(lab_19>50, na.rm=TRUE)\n    )\n## # A tibble: 1 x 6\n## min_swing max_swing median_swing num_swing num_land_con num_land_lab\n##     <dbl>    <dbl>        <dbl>     <int>        <int>        <int>\n## 1   -6.47     18.4        4.44       599          280          120\n```\n:::\n\n\n\n\n\n### Plot distributions\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Histograms of Butler two-party Labour-Conservative Swing.](figs/03/hist.png){#fig-hist width=90%}\n:::\n:::\n\n\n\n\n\nLet's start with ggplot2 specifications by plotting some of these variables. Below is the code for plotting a histogram of the Swing variable.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_gb |>\n  ggplot(mapping=aes(swing_con_lab)) +\n  geom_histogram()\n```\n:::\n\n\n\n\n\nA reminder of the general form of a ggplot2 specification:\n\n1.  Start with some *data*: `data_gb`.\n2.  Define the *encoding*: `mapping=aes()` into which we pass the `swing_con_lab` variable.\n3.  Specify the *marks* to be used: `geom_histogram()` in this case.\n\nDifferent from the scatterplot example, there is more happening in the internals of ggplot2 when creating a histogram. The Swing variable is partitioned into bins, and observations in each bin are counted. The x-axis (bins) and y-axis (counts by bin) are derived from the `swing_con_lab` variable.\n\n\\index{distributional plots!histograms} \\index{ggplot2!mapping \\texttt{aes()}}\nBy default the histogram's bars are given a grey colour. To *set* them to a different colour, add a `fill=` argument to `geom_histogram()`. In the code block below, colour is set using hex codes. The term *set*, not *map* or *encode*, is used for principled reasons. Any part of a ggplot2 specification that involves encoding data -- mapping a data item to a visual channel -- should be specified through the `mapping=aes()` argument. Anything else, for example changing the default colour, thickness and transparency of marks, needs to be *set* outside of this argument.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_gb |>\n  ggplot(mapping=aes(swing_con_lab)) +\n  geom_histogram(fill=\"#003c8f\") +\n  labs(x=\"Swing\", y=\"count\")\n```\n:::\n\n\n\n\n\n\\index{ggplot2!faceting|(} \\index{ggplot2!scales}\nYou will notice that different elements of a ggplot2 specification are added (`+`) as layers. In the example above, the additional layer of labels (`labs()`) is not intrinsic to the graphic. However, often you will add layers that do affect the graphic itself. For example, the scaling of encoded values (e.g. `scale_*_continuous()`) or whether the graphic is to be conditioned on another variable to generate small multiples for comparison (e.g. `facet_*()`). Adding a call to `facet_*()`, we can compare how Swing varies by region (@fig-hist-region). The plot is annotated with the *median* value for Swing (4.4) by adding a vertical line layer (`geom_vline()`) set with an x-intercept at this median value. From this, there is some evidence of a regional geography to the 2019 vote: London and Scotland are distinctive in containing relatively few constituencies swinging greater than the expected midpoint; North East, Yorkshire & The Humber, and to a lesser extent West and East Midlands, appear to show the largest relative number of constituencies swinging greater than the midpoint.\n\n\n\n\n\n\n```{=html}\n<!-- ::: {.callout-note}\n## On histograms\nThis design exposition by [Lunzner and McNamara, 2020](http://tinlizzie.org/histograms/) is an excellent discussion of the analysis and design considerations when working with histograms. There are of course other *geoms* for summarising over 1D distributions: `geom_boxplot()`, `geom_dotplot()`, `geom_violin()`.\n::: -->\n```\n\n::: {.cell}\n::: {.cell-output-display}\n![Histograms of Swing variable, grouped by region.](figs/03/hist-region.png){#fig-hist-region width=90%}\n:::\n:::\n\n\n\n\n\n\\index{Swing|)}\n\n::: {.callout-tip icon=\"false\"}\n## Task 2\n\nUpdate the earlier ggplot2 specification to produce a set of histograms of the Swing variable faceted by region, similar to that in @fig-hist-region.\n:::\n\n### Plot ranks/magnitudes\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Plots of vote shares by party.](figs/03/bars.png){#fig-bars-party width=80%}\n:::\n:::\n\n\n\n\n\n\n\nPreviously we calculated overall vote shares by political party. We could continue the exploration of votes by region, re-using this code to generate plots displaying vote shares by region, using marks and encoding channels that are suitable for magnitudes.\n\nTo generate a bar chart similar to @fig-bars-party the ggplot2 specification would be:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_gb |>\n  # The code block summarising vote by party.\n  <some dplyr code> |>\n  # Ordinal x-axis (party, reordered), Ratio y-axis (vote_share).\n  ggplot(aes(x=reorder(party, -vote_share), y=vote_share)) +\n  geom_col(fill=\"#003c8f\") +\n  coord_flip()\n```\n:::\n\n\n\n\n\nA quick breakdown of the specification:\n\n1.  *Data*: This is the summarised data frame in which each row is a political party, and the column describes the vote share recorded for that party.\n2.  *Encoding*: We have dropped the call to `mapping=`. ggplot2 always looks for `aes()`, and so we can save on code clutter. In this case we are mapping `party` to the x-axis, a categorical variable made ordinal by the fact that we reorder the axis left-to-right descending on `vote_share`. `vote_share` is mapped to the y-axis -- so encoded using bar length on an aligned scale, an effective channel for conveying magnitudes.\n3.  *Marks*: `geom_col()` for generating the bars.\n4.  *Setting*: Again, we've set bar colour to manually selected dark blue. Optionally we add a `coord_flip()` layer in order to display the bars horizontally. This makes the category axis labels easier to read and also seems more appropriate for the visual \"ranking\" of bars.\n\n\n\n\n\n```{=html}\n<!-- ::: {.callout-note}\n## ggplot2 themes\nggplot2 themes control the appearance of all non-data items -- font sizes and types, gridlines, axis labels. Checkout the complete list of ggplot2's [default themes](https://ggplot2.tidyverse.org/reference/ggtheme.html). If you like the look of the BBC's in-house data graphics, explore their [Data Journalism cookbook](https://bbc.github.io/rcookbook/), a great resource for distilling many of the non-data-related decisions to be made when communicating graphically.\n::: -->\n```\n\n\n\n\n#### Faceting by region {.unnumbered}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Plots of vote shares by party and region.](figs/03/bars-region.png){#fig-bars-region width=100%}\n:::\n:::\n\n\n\n\n\n\nIn @fig-bars-region the graphic is faceted by region. This requires an updated staged dataset grouping by `vote_share` *and* `region` and of course a faceting layer (`geom_facet(~region)`). The graphic is more data-rich, and additional cognitive effort is required in relating the political party bars between different graphical subsets. We can assist this associative task by encoding parties with an appropriate visual channel: *colour hue*. The ggplot2 specification for this is as you would expect; we add a mapping to `geom_col()` and pass the variable name `party` to the fill argument (`aes(fill=party)`).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_gb |>\n  # The code block summarising vote by party and also now region.\n  <some dplyr code> |>\n  # To be piped to ggplot2.\n  ggplot(aes(x=reorder(party, vote_share), y=vote_share)) +\n  geom_col(aes(fill=party)) +\n  coord_flip() +\n  facet_wrap(~region)\n```\n:::\n\n\n\n\n\nTrying this for yourself, you will observe that the ggplot2 internals do some thinking for us. Since `party` is a categorical variable, a categorical hue-based colour scheme is automatically applied. Try passing a quantitative variable (`fill=vote_share`) to `geom_col()` and see what happens; a quantitative colour gradient scheme is applied.\n\nClever as this is, when encoding political parties with colour, *symbolisation* is important. It makes sense to represent political parties using colours with which they are most commonly associated. We can override ggplot2's default colour by adding a `scale_fill_manual()` layer into which a vector of hex codes describing the colour of each political party is passed (`party_colours`). We also need to tell ggplot2 which element of `party_colours` to apply to which value of the `party` variable. In the code below, a staging table is generated summarising `vote_share` by political party and region. In the final line the `party` variable is recoded as a `factor`. You might recall from the last chapter that factors are categorical variables of fixed and orderable values -- `levels`. The call to `mutate()` recodes `party` as a factor variable and orders the levels according to overall vote share. \\index{code!factors} \\index{colour}\\index{symbolisation}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate staging data.\ntemp_party_shares_region <- data_gb |>\n  select(\n    constituency_name, region, total_vote_19, \n    con_vote_19:alliance_vote_19\n    ) |>\n  pivot_longer(\n    cols=con_vote_19:alliance_vote_19, \n    names_to=\"party\", values_to=\"votes\"\n    ) |>\n  mutate(party=str_extract(party, \"[^_]+\")) |>\n  group_by(party, region) |>\n  summarise(vote_share=sum(votes, na.rm=TRUE)/sum(total_vote_19)) |>\n  filter(\n    party %in% c(\"con\", \"lab\", \"ld\", \"snp\", \"green\", \"brexit\", \"pc\")\n    ) |>\n  mutate(party=factor(party,\n        levels=c(\"con\", \"lab\", \"ld\", \"snp\", \"green\", \"brexit\", \"pc\"))\n      )\n```\n:::\n\n\n\n\n\nNext, a vector of objects is created containing the hex codes for the colours of political parties (`party_colours`). \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define colours.\ncon <- \"#0575c9\"\nlab <- \"#ed1e0e\"\nld <- \"#fe8300\"\nsnp <- \"#ebc31c\"\ngreen <- \"#78c31e\"\npc <- \"#4e9f2f\"\nbrexit <- \"#25b6ce\"\nother <- \"#bdbdbd\"\n\nparty_colours <- c(con, lab, ld, snp, green, brexit, pc)\n```\n:::\n\n\n\n\n\nThe ggplot2 specification is then updated with the `scale_fill_manual()` layer:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp_party_shares_region |>\n  ggplot(aes(x=reorder(party, vote_share), y=vote_share)) +\n  geom_col(aes(fill=party)) +\n  scale_fill_manual(values=party_colours) +\n  coord_flip() +\n  facet_wrap(~region)\n```\n:::\n\n\n\n\n\n\\index{ggplot2!faceting|)}\n\n::: callout-note\n## Grammar of Graphics-backed visualization toolkits\n\n\\index{Grammar of Graphics}\\index{interactive data analysis}\n\n| The idea behind visualization toolkits such as ggplot2 is to insert visual approaches into a data scientist's workflow. Rather than being overly concerned with low-level aspects of drawing, mapping data values to screen coordinates and scaling factors, you instead focus on aspects relevant to the analysis -- the variables in a dataset and how they will be encoded and conditioned using visuals. Hadley Wickham talks about a grammar of *interactive* data analysis, whereby `dplyr` functions are used to rapidly prepare data for charting before being piped (`|>`) to ggplot2. \n| \n| The process of searching for, defining and inserting manual colour schemes for creating @fig-bars-region might seem inimical to this. There is some reasonably involved `dplyr` and a little regular expression in the data preparation code that you should not be overly concerned with. Having control of these slightly more low-level properties is, though, sometimes necessary even for exploratory analysis, in this case for effecting *symbolisation* that supports comparison.\n:::\n\n### Plot relationships\n\n\\index{multivariate plots!scatterplots|(}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Plots of 2019 versus 2017 vote shares.](figs/03/scatters-con.png){#fig-scatters-con width=85%}\n:::\n:::\n\n\n\n\n\n\n\nTo continue the investigation of change in votes for the major parties between 2017 and 2019, @fig-scatters-con contains a scatterplot of Conservative vote share in 2019 (y-axis) against vote share in 2017 (x-axis). The graphic is annotated with a diagonal line. If constituencies voted in 2019 in exactly the same way as 2017, the points would converge on the diagonal. Points above the diagonal indicate a larger Conservative vote share than 2017, those below the diagonal a smaller Conservative vote share than 2017. Points are coloured according to the winning party in 2019, and constituencies that flipped from Labour to Conservative are emphasised using transparency and shape.\n\nThe code for generating most of the scatterplot in @fig-scatters-con is below.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_gb |>\n  mutate(winner_19=case_when(\n           winner_19 == \"Conservative\" ~ \"Conservative\",\n           winner_19 == \"Labour\" ~ \"Labour\",\n           TRUE ~ \"Other\"\n         )) |>\n  ggplot(aes(x=con_17, y=con_19)) +\n  geom_point(aes(colour=winner_19), alpha=.8) +\n  geom_abline(intercept = 0, slope = 1) +\n  scale_colour_manual(values=c(con,lab,other)) +\n  ...\n```\n:::\n\n\n\n\n\nThere is little surprising here:\n\n1.  *Data*: The `data_gb` data frame. Values of `winner_19` that are not *Conservative* or *Labour* are recoded to *Other* using a conditional statement. This is to ease and narrow the comparison to the two major parties. \n2.  *Encoding*: Conservative vote shares in 2017 and 2019 are mapped to the x- and y- axes respectively and `winner_19` to colour. `scale_colour_manual()` is used for customising the colours.\n3.  *Marks*: `geom_point()` for generating the points of the scatterplot; `geom_abline()` for drawing the reference diagonal.\n\n::: {.callout-tip icon=\"false\"}\n## Task 3\n\n| The code block above doesn't exactly reproduce the graphic in @fig-scatters-con. Try updating the ggplot2 specification to emphasise constituencies that flipped from Labour to Conservative. \n| \n| Hint: you may wish to generate a variable recording constituencies that flipped between 2017 and 2019 and encode some visual channel in the graphic on this.\n:::\n\n\\index{multivariate plots!scatterplots|)}\n\n### Plot geography\n\n\\index{maps!choropleth maps} \n\nThe data graphics above suggest that the composition of Conservative and Labour voting may be shifting. Paying attention to the geography of voting, certainly to *changes* in voting between 2017 and 2019 elections (e.g. @fig-hist-region), may therefore be instructive. We end the technical component to the chapter by generating thematic maps of the results data.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Choropleth of elected parties in 2019 General Election.](figs/03/map-winners.png){#fig-map-winners width=100%}\n:::\n:::\n\n\n\n\n\n\nTo do this we need to generate a join on the boundary dataset loaded at the start of this technical section (`cons_outline`):\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join constituency boundaries.\ndata_gb <- cons_outline |>\n  inner_join(data_gb, by=c(\"pcon21cd\"=\"ons_const_id\"))\n# Check class.\n## [1] \"sf\"         \"data.frame\"\n```\n:::\n\n\n\n\n\nThe code for generating the choropleth maps of winning party by constituency in @fig-map-winners:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Recode winner_19 as a factor variable for assigning colours.\ndata_gb <- data_gb |>\n  mutate(\n    winner_19=if_else(winner_19==\"Speaker\", \"Other\", winner_19),\n    winner_19=factor(winner_19, levels=c(\"Conservative\", \"Labour\", \n    \"Liberal Democrat\", \"Scottish National Party\", \"Green\", \n    \"Plaid Cymru\", \"Other\"))\n     )\nparty_colours <- c(con, lab, ld, snp, green, pc, other)\n# Plot map.\ndata_gb |> \n  ggplot() +\n  geom_sf(aes(fill=winner_19), colour=\"#eeeeee\", linewidth=0.01) +\n  # Optionally add a layer for regional boundaries.\n  geom_sf(data=. %>% group_by(region) %>% summarise(),\n      colour=\"#eeeeee\", fill=\"transparent\", linewidth=0.08) +\n  coord_sf(crs=27700, datum=NA) +\n  scale_fill_manual(values=party_colours)\n```\n:::\n\n\n\n\n\nA breakdown of the ggplot2 spec:\n\n1.  *Data*: Update `data_gb` by recoding `winner_19` as a factor and defining a named vector of colours to supply to `scale_fill_manual()`. Note that we also use the `party_colours` object created for the region bar chart.\n2.  *Encoding*: No surprises here -- `fill` according to `winner_19`.\n3.  *Marks*: `geom_sf()` is a special class of geometry. It draws objects using the contents of a simple features data frame's [@pebesma_simple_2018] `geometry` column. In this case `MULTIPOLYGON`, so read this as a polygon shape primitive. \\index{packages!\\texttt{simple features}}\n4.  *Coordinates*: `coord_sf` -- we set the coordinate system (CRS) explicitly. In this case OS British National Grid. \\index{maps!OS British National Grid}\n5.  *Setting*: Constituency boundaries are subtly introduced by setting the `geom_sf()` mark to light grey (`colour=\"#eeeeee\"`) with a thin outline (`linewidth=0.01`). On the map to the right, outlines for regions are added as another `geom_sf()` layer. Note how this is achieved in the second `geom_sf()`. The `data_gb` dataset initially passed to ggplot2 (identified by the `.` mark) is collapsed by region (with `group_by()` and `summarise()`) and in the background the boundaries in `geometry` are aggregated by region.\n\n::: callout-note\n## Preparing data for plotting\n\nA general point from the code blocks in this chapter is that proficiency in `dplyr` and `tidyr` is a necessity. Throughout the book you will find yourself needing to calculate new variables, recode variables and reorganise data frames before handing them over to ggplot2 for plotting.\n:::\n\nIn the third map of @fig-map-winners the transparency (`alpha`) of filled constituencies is varied according to the Swing variable. This demonstrates that constituencies swinging most dramatically for Conservative (darker colours) are in the midlands and North of England and not in London and the South East. The pattern is nevertheless a subtle one; transparency (colour luminance / saturation) is not a highly effective visual channel for encoding quantities. \n\n\\index{maps!glyphmaps|(}\n\nIt may be worth applying the same encoding to Butler two-party swing as that used in the Washington Post graphic when characterising Republican-Democrat swing in 2016 US Elections [e.g. @beecham_using_2020]. This can be achieved by simply adding another ggplot2 layer, though the code is a little more involved. ggplot2's `geom_spoke()` primitive draws line segments parameterised by a location (x- y- position) and angle. With this we can encode constituencies with [\\|]{style=\"font-weight:bolder\"} marks that angle to the right [/]{style=\"color:#3879A1;font-weight:bold\"} where the constituency swings towards Conservative and to the left where it swings towards Labour [\\\\]{style=\"color:#DB534D;font-weight:bolder\"}. This encoding better exposes the pattern of constituencies forming Labour's \"red wall\" in the north of England, as well as parts of Wales and the Midlands flipping to Conservative.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Map of Butler Con-Lab Swing in 2019 General Election.](figs/03/spoke-map.png){#fig-spoke-map width=90%}\n:::\n:::\n\n\n\n\n\n\nThe ggplot2 specification:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find the maximum Swing values to pin the min and max angles to.\nmax_shift <- max(abs(data_gb |> pull(swing_con_lab)))\nmin_shift <- -max_shift\n\n# Re-define party_colours to contain just three values: hex codes for\n# Conservative, Labour and Other.\nparty_colours <- c(con, lab, other)\nnames(party_colours) <- c(\"Conservative\", \"Labour\", \"Other\")\n\n# Plot Swing map.\ndata_gb |>\n  mutate(\n    is_flipped=seat_change_1719 %in%\n       c(\"Conservative gain from Labour\",\n       \"Labour gain from Conservative\"),\n    elected=if_else(!winner_19 %in% c(\"Conservative\", \"Labour\"), \"Other\",\n       as.character(winner_19)),\n       swing_angle=\n        get_radians(map_scale(swing_con_lab,min_shift,max_shift,135,45)\n      )\n   ) |>\n  ggplot()+\n  geom_sf(aes(fill=elected), colour=\"#636363\", alpha=.2, linewidth=.01)+\n  geom_spoke(\n    aes(x=bng_e, y=bng_n, angle=swing_angle, colour=elected, \n      linewidth=is_flipped),\n    radius=7000, position=\"center_spoke\"\n    )+\n  coord_sf(crs=27700, datum=NA)+\n  scale_linewidth_ordinal(range=c(.2,.5))+\n  scale_colour_manual(values=party_colours)+\n  scale_fill_manual(values=party_colours)\n```\n:::\n\n\n\n\n\nA breakdown:\n\n1.  *Data*: `data_gb` is updated with a Boolean (`TRUE`/`FALSE`) variable identifying whether or not the constituency flipped between successive elections (`is_flipped`), and a variable simplifying the party elected to either Conservative, Labour or Other. `swing_angle` contains the angles used to orient the line marks. A convenience function (`map_scale()`) pins the maximum swing values to 45 degrees and 135 degrees -- respectively max swing to the right, Conservative and max swing to the left, Labour.\n2.  *Encoding*: `geom_sf()` is again filled by elected party. This encoding is made more subtle by adding transparency (`alpha=.2`). `geom_spoke()` is mapped to the geographic centroid of each Constituency (`bng_e` - easting, `bng_n` - northing), coloured on elected party, sized on whether the constituency flipped its vote and tilted or angled according to the `swing_angle` variable.\n3.  *Marks*: `geom_sf()` for the constituency boundaries, `geom_spoke()` for the angled line primitives.\n4.  *Scale*: `geom_spoke()` primitives are sized to emphasise whether constituencies have flipped. The size encoding is censored to two values with `scale_linewidth_ordinal()`. Passed to `scale_colour_manual()` and `scale_fill_manual()` is the vector of `party_colours`.\n5.  *Coordinates*: `coord_sf` -- the CRS is OS British National Grid, so we define constituency centroids using easting and northing planar coordinates.\n6.  *Setting*: The `radius` of `geom_spoke()` lines is a sensible default arrived at through trial and error, its `position` set using a newly created `center_spoke` class.\n\nThere are helper functions that must also be run to execute the ggplot2 code above correctly. In order to position lines using `geom_spoke()` centred on their x- y- location, we need to create a custom ggplot2 subclass. Details are in the `03-template.qmd` file. Again, this is somewhat involved for a chapter introducing ggplot2 for analysis. Nevertheless, hopefully you can see from the plot specification above that the principles of mapping data to visuals can be implemented straightforwardly in ggplot2: line marks for constituencies (`geom_spoke()`), positioned in `x` and `y` according to British National Grid easting and northings and oriented (`angle`) according to two-party Swing.\n\n\\index{maps!glyphmaps|)}\n\n#### Dot-density maps {.unnumbered}\n\n\\index{maps!dot-density maps|(}\n\nA common design challenge when presenting population data spatially is to accurately reflect *geography* at the same time as the *quantitative* outcome of interest -- in this case, the location and shape of constituencies versus their associated vote sizes.  You may be familiar with cartogram layouts used in electoral analysis. They are maps that distort geographic space in order to size constituencies according to the voting population rather than their physical area. Dot-density maps also convey absolute numbers of votes but in a way that preserves geography. In the example below, each dot represents 1,000 votes for a given party -- Conservative, Labour, Other -- and dots are positioned in the constituencies from which those votes were made. Dots therefore concentrate in population-dense areas of the country. \n\nThe difficulty in generating dot-density maps is not wrangling ggplot2, but in preparing data to be plotted. We need to create a randomly located point within a constituency's boundary for every thousand votes that are made there.  R packages specialised to dot-density maps provide functions for doing this, but it is quite easy to achieve using the sorts of functional and `tidyverse`-style code introduced throughout this book. We include the code for @fig-dot-density directly below the plot. While compact, there are some quite advanced functional programming concepts (in the use of `purrr::map()`) that we do not explain. These concepts are in fact covered in some detail and with proper description in later chapters of the book.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Dot density map of 2019 General Election result.](figs/03/dot_density.png){#fig-dot-density width=80%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# Collect 2019 GE data from which dots are approximated.\nvote_data <- bes_2019 |>   \n  filter(ons_const_id!=\"S14000051\") |> \n  mutate(\n    other_vote_19=total_vote_19-(con_vote_19 + lab_vote_19)\n    ) |> \n  select(\n    ons_const_id, constituency_name, region, con_vote_19, \n    lab_vote_19, other_vote_19\n    ) |> \n  pivot_longer(\n    cols=con_vote_19:other_vote_19, \n    names_to=\"party\", values_to=\"votes\"\n    ) |> \n  mutate(\n    party=str_extract(party, \"[^_]+\"),\n    votes_dot=round(votes/1000,0)\n    ) |>  \n  filter(!is.na(votes_dot))\n\n# Sample within constituency polygons. \n# This might take a bit of time to execute.\nstart_time <- Sys.time()\nsampled_points <- \n  cons_outline |>   \n  select(geometry, pcon21cd) |> filter(pcon21cd!=\"S14000051\") |>  \n  inner_join(\n    vote_data |> group_by(ons_const_id) |>  \n    summarise(votes_dot=sum(votes_dot)) |>  ungroup(), \n    by=c(\"pcon21cd\"=\"ons_const_id\")\n    ) |> \n  nest(data=everything()) |> \n  mutate(\n    sampled_points=map(data, \n      ~sf::st_sample(\n        x=.x, size=.x$votes_dot, exact=TRUE, type=\"random\"\n        ) |> st_coordinates() |> \n        as_tibble(.name_repair=~c(\"east\", \"north\"))),\n     const_id=map(data, ~.x |>  st_drop_geometry() |> \n     select(pcon21cd, votes_dot) |> uncount(votes_dot))\n    ) |> \n  unnest(-data) |> \n  select(-data)\nend_time <- Sys.time()\nend_time - start_time\npoint_votes <- vote_data |> select(party, votes_dot) |> \n  uncount(votes_dot)\nsampled_points  <- sampled_points |>  bind_cols(point_votes)\n\n# Plot sampled points.\nparty_colours <- c(con, lab, other)\nsampled_points |> \n  ggplot() +\n  geom_sf(\n    data=cons_outline, fill=\"transparent\", \n    colour=\"#636363\", linewidth=.03\n    ) +\n  geom_sf(data=cons_outline |> \n      inner_join(vote_data, by=c(\"pcon21cd\"=\"ons_const_id\")) |> \n      group_by(region) |>  summarise(),\n    fill=\"transparent\", colour=\"#636363\", linewidth=.1) +\n  geom_point(\n    aes(x=east,y=north, fill=party, colour=party), \n    alpha=.5, size=.6, stroke=0\n    )+\n  scale_fill_manual(values=party_colours, \"1 dot = 1,000 votes\")+\n  scale_colour_manual(values=party_colours, \"1 dot = 1,000 votes\")+\n  guides(colour=guide_legend(override.aes=list(size=3)))+\n  theme_void() \n```\n:::\n\n\n\n\n\n\n\\index{maps!dot-density maps|)} \\index{datasets!UK General Election|)} \\index{UK General Election|)}\n\n## Conclusions\n\nVisualization design is ultimately a process of decision-making. Data must be filtered and prioritised before being encoded with marks, visual channels and symbolisation. The most successful data graphics are those that expose structure, connections and comparisons that could not be achieved easily via other, non-visual means. This chapter has introduced concepts -- a vocabulary, framework and empirically-informed guidelines -- that help support this decision-making and that underpin modern visualization toolkits, ggplot2 especially. Through an analysis of UK 2019 General Election data, we have demonstrated how these concepts can be applied in a real data analysis.\n\n## Further Reading\n\nFor a primer on visualization design principles:\n\n-   Munzner, T. 2014. \"Visualization Analysis and Design\", Boca Raton, FL: *CRC Press*.\n\nA paper presenting evidence-backed guidelines on visualization design, aimed at applied researchers:\n\n-   Franconeri S. L., Padilla L. M., Shah P., Zacks J. M., Hullman J. (2021). \"The science of visual data communication: What works\". *Psychological Science in the Public Interest*, 22(3), 110--161. doi: 10.1177/15291006211051956.\n\nFor an introduction to ggplot2 and its relationship with Wilkinson's [-@wilkinson_grammar_1999] *grammar of graphics*:\n\n-   Wickham, H., etinkaya-Rundel, M., Grolemund, G. 2023, \"R for Data Science, 2nd Edition\", Sebastopol, CA: *O'Reilly*.\n    -   Chapters 1, 9.\n\nExcellent paper looking at consumption and impact of election forecast visualizations:\n\n-   Yang, F. et al. 2024. \"Swaying the Public? Impacts of Election Forecast Visualizations on Emotion, Trust, and Intention in the 2022 U.S. Midterms.\" *IEEE Transactions on Visualization and Computer Graphics*, 30(1), 23--33. doi: 10.1109/TVCG.2023.3327356.\n\n",
    "supporting": [
      "03-visual_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}